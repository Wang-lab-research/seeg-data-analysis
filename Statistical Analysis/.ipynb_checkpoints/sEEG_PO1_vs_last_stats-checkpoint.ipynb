{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14889b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_26119/3243943048.py:21: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as scio\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mne.time_frequency import tfr_multitaper\n",
    "from mne.stats import permutation_cluster_1samp_test as pcluster_test\n",
    "import scipy.stats as stats\n",
    "from statannotations.Annotator import Annotator\n",
    "%matplotlib inline\n",
    "# sns.set(style=\"whitegrid\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b18f6c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "501d8e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_multiple_elements(list_object, indices):\n",
    "    indices = sorted(indices, reverse=True)\n",
    "    for idx in indices:\n",
    "        if idx < len(list_object):\n",
    "            list_object.pop(idx)## define functions for extracting relevant epochs\n",
    "            \n",
    "def stats_plot(df, save_dir, feature, file_name):\n",
    "    '''\n",
    "    df = dataframe (mean, peak)\n",
    "    save_dir = directory to save stats PNG and TXT files\n",
    "    feature = 'mean','peak'\n",
    "    '''    \n",
    "\n",
    "    pairs=[((f,conditions[0]),(f,conditions[1])) for f in freq_bands_of_interest]\n",
    "\n",
    "    for roi in roi_names:        \n",
    "        df_ch_tmp=df.query(f\"channel=='{roi}'\")\n",
    "       \n",
    "\n",
    "        #stats plot\n",
    "        plt.figure(figsize=(8.6,6.5));\n",
    "        ax_tmp = sns.boxplot(data=df_ch_tmp,x='band', y='value', hue='condition',\n",
    "                palette='pastel', order=freq_bands_of_interest,\n",
    "                hue_order=conditions,\n",
    "                linewidth=0.5)\n",
    "\n",
    "        ax_tmp.set_title(f\"{roi} | Resting state Power | Feature: '{feature}' \",pad=30)\n",
    "\n",
    "        #stats output\n",
    "        x='band'\n",
    "        y='value'\n",
    "        hue='condition'\n",
    "        order=freq_bands_of_interest\n",
    "        hue_order = conditions\n",
    "\n",
    "        test = 'Mann-Whitney'\n",
    "        print(f\"******************************************************{roi} {test}******************************************************\")\n",
    "        annot = Annotator(ax_tmp, pairs, data=df_ch_tmp, x=x, y=y, hue=hue, order=order,hue_order=hue_order)\n",
    "        annot.configure(test=test, text_format='star', loc='outside',comparisons_correction=None)\n",
    "        annot.apply_test()\n",
    "        annot.annotate()\n",
    "\n",
    "        # save plot and tfr data\n",
    "        # TODO: follow same naming convention as in preprocessing code\n",
    "        # Add region to end, removed raw.fif\n",
    "        save_fname = f\"{file_name[:-4]}_{roi}\"\n",
    "        \n",
    "        # plot as png\n",
    "        plt.savefig(os.path.join(save_dir,sub_folder,save_fname+\".png\"))\n",
    "        plt.figure()\n",
    "        \n",
    "        # tfr data as csv\n",
    "        df_ch_tmp.to_csv(os.path.join(save_dir,sub_folder,save_fname+\".csv\"))\n",
    "        display.clear_output(wait=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5f6060b-2c9f-4fb3-ba2f-b6815a066be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions are divided into surface and depth generators, surface generator allows users to select chosen nodes given image data\n",
    "# depth generator used for when images can offer no better info on channel location\n",
    "\n",
    "def ROI_Surface_Generator(ROI_Data_Path, ROI_sub_id, surface_side): # surface side must be 'lh' (lefthand) or 'rh' (righthand)\n",
    "    for file in os.listdir(ROI_Data_Path):\n",
    "        if str(ROI_sub_id) in file and not str('depth') in file and str(surface_side) in file:\n",
    "            processed_roi_csv = pd.read_csv(os.path.join(ROI_Data_Path, file))\n",
    "\n",
    "    ROI_channels = processed_roi_csv['ID'].to_list()\n",
    "    Chosen_ROI_Channels = [] # reset\n",
    "    channel_ids = 0\n",
    "    \n",
    "    while channel_ids != len(ROI_channels):\n",
    "        verification_status = input(f\"Proposed Channel in Region of Interest: {ROI_channels[channel_ids]} - Verify from Image, Keep y/n ?\")\n",
    "        if verification_status == 'y':\n",
    "            Chosen_ROI_Channels.append(ROI_channels[channel_ids])\n",
    "            channel_ids = channel_ids + 1\n",
    "            display.clear_output(wait=True)\n",
    "        elif verification_status == 'n':\n",
    "            channel_ids = channel_ids + 1\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            print(\"Invalid Reponse, please enter 'y' or 'n'\")\n",
    "            channel_ids = channel_ids + 0\n",
    "            display.clear_output(wait=True)\n",
    "            \n",
    "    return(Chosen_ROI_Channels) # returns chosen channels as list of channel IDs\n",
    "\n",
    "\n",
    "\n",
    "def ROI_Depth_Generator(ROI_Data_Path, ROI_sub_id): # use for files containing 'depth'\n",
    "    for file in os.listdir(ROI_Data_Path):\n",
    "        if str(ROI_sub_id) in file and str('depth') in file:\n",
    "            processed_roi_csv = pd.read_csv(os.path.join(ROI_Data_Path))\n",
    "    \n",
    "    ROI_channels = processed_roi_csv['ID'].to_list()\n",
    "    return(ROI_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e305a7c4-a470-4122-ad9d-b12f212af5fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4f713ab-360e-43ca-9520-a14c9b5f399c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "735eea71-ab17-44b8-8267-cb3f73061d12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (4105867405.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[10], line 10\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"Adding {file} to analyze\u001b[0m\n\u001b[0m                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "# TODO: Create epoch generator that takes the chosen ROI channels and uses them to generate epochs and compute ave / normalized psds\n",
    "# combine (concat) the final avg psds from both data files from the same sub state to create one avg psd for each sub state (awake / asleep)\n",
    "\n",
    "\n",
    "def epoch_generator(file_list, picks, sub_state, sub_id):\n",
    "    \n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ LOAD SUBJECT DATA @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #    \n",
    "    \n",
    "    for i in range(0, len(file_list)):\n",
    "        raw_fname = os.path.join(processed_info_dir, str(file_list[i], notch_widths = 10)\n",
    "        print(f\"analyzing data for {raw_fname[:-8]}\"\n",
    "        \n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ COMPUTE PSD FROM RAW FIF @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #        \n",
    "        \n",
    "        raw = mne.io.read_raw_fif(raw_fname).crop(tmin, tmax).load_data()\n",
    "        raw = raw.notch_filter(np.arange(60, 481, 60), n_jobs=-1, fir_design='firwin') # inital notch filter for all harmonics\n",
    "        epochs = mne.make_fixed_length_epochs(raw, duration=30, preload=False)\n",
    "        n_fft = 2048  # the FFT size (n_fft). Ideally a power of 2\n",
    "              \n",
    "        # Calculate the psds from raw data based on frequency range\n",
    "        broad1 = epochs.compute_psd(fmin=1., fmax=55., picks=picks, n_jobs=n_cores) \n",
    "        delta = epochs.compute_psd(fmin=1., fmax=4., picks=picks, n_jobs=n_cores)\n",
    "        theta = epochs.compute_psd(fmin=4., fmax=8., picks=picks, n_jobs=n_cores)\n",
    "        alpha = epochs.compute_psd(fmin=8., fmax=13., picks=picks, n_jobs=n_cores)\n",
    "        beta = epochs.compute_psd(fmin=13., fmax=30., picks=picks, n_jobs=n_cores)\n",
    "        low_gamma = epochs.compute_psd(fmin=30., fmax=55., picks=picks, n_jobs=n_cores)\n",
    "        high_gamma = epochs.compute_psd(fmin=65., fmax=100. picks=picks,, n_jobs=n_cores)\n",
    "\n",
    "        # Get average power across bands (shape = epochs x channels)\n",
    "        avg_psd_broad1 = np.mean(broad1, axis=2)\n",
    "        avg_psd_delta = np.mean(delta, axis=2)\n",
    "        avg_psd_theta = np.mean(theta, axis=2)\n",
    "        avg_psd_alpha = np.mean(alpha, axis=2)\n",
    "        avg_psd_beta = np.mean(beta, axis=2)\n",
    "        avg_psd_low_gamma = np.mean(low_gamma, axis=2)\n",
    "        avg_psd_high_gamma = np.mean(high_gamma, axis=2)\n",
    "              \n",
    "        # Combine broad spectrum powers and take the trace\n",
    "        avg_psd_broad = np.mean( np.array([avg_psd_broad1, avg_psd_broad2 ]), axis=0 )\n",
    "        trace_avg_psd_broad = np.trace(avg_psd_broad)\n",
    "        print(f\"Trace of the average broad spectrum power: {trace_avg_psd_broad}\")    \n",
    "        \n",
    "        # Get band PSD's normalized by the trace of the broad spectrum and convert to dB\n",
    "        avg_psd_delta = 10 * np.log10(avg_psd_delta / trace_avg_psd_broad)\n",
    "        avg_psd_theta = 10 * np.log10(avg_psd_theta / trace_avg_psd_broad)\n",
    "        avg_psd_alpha = 10 * np.log10(avg_psd_alpha / trace_avg_psd_broad)\n",
    "        avg_psd_beta = 10 * np.log10(avg_psd_beta / trace_avg_psd_broad)\n",
    "        avg_psd_low_gamma = 10 * np.log10(avg_psd_low_gamma / trace_avg_psd_broad)\n",
    "        avg_psd_high_gamma = 10 * np.log10(avg_psd_high_gamma / trace_avg_psd_broad)\n",
    "        avg_psd_MUA = 10 * np.log10(avg_psd_MUA / trace_avg_psd_broad)\n",
    "        \n",
    "        avg_psd_arr = np.array((avg_psd_delta, avg_psd_theta, \n",
    "                                avg_psd_alpha, avg_psd_beta,\n",
    "                                avg_psd_low_gamma, avg_psd_high_gamma, avg_psd_MUA)) \n",
    "              \n",
    "        avg_psd_arr_lst = avg_psd_arr_lst.append(avg_psd_arr)\n",
    "    \n",
    "    ID_sub_state_psd_arr = np.concatenate(avg_psd_arr_lst)\n",
    "    return(ID_sub_state_psd_arr)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "78762c37-4bdf-443c-bf89-0b0b3dbde607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automate the generation of broad or MUA (>100 Hz) psds and combine them\n",
    "\n",
    "def notch_filter_analysis(broad_or_MUA, generated_epochs, avg_psd_broad_lower): # enter 'broad' or 'MUA' for broad_or_MUA argument\n",
    "    if broad_or_MUA == 'broad':\n",
    "        broad_band_list = [] # reset\n",
    "        for i in range(0, (int(MUA_max/60)-1)):\n",
    "            if i != (int(MUA_max/60)-1):\n",
    "                broad_band = generated_epochs.compute_psd(fmin=65+i*60., fmax=55+(i+1)*60., picks='all', n_jobs=n_cores)\n",
    "                avg_psd_broad_band = np.mean(broad_band, axis=2)\n",
    "                broad_band_list.append(avg_psd_broad_band)\n",
    "            else:\n",
    "                broad_band = generated_epochs.compute_psd(fmin=65+i*60., fmax = MUA_max, picks = 'all', n_jobs=n_cores)\n",
    "                avg_psd_broad_band = np.mean(broad_band, axis=2)\n",
    "                broad_band_list.append(avg_psd_broad_band)\n",
    "        broad_band_list.append(avg_psd_broad_lower)\n",
    "        avg_psd_broad = np.mean(np.array(broad_band_list), axis=0)\n",
    "        return(avg_psd_broad)\n",
    "    elif broad_or_MUA == 'MUA':\n",
    "        MUA_band_list = [] # reset\n",
    "        for i in range(0, (int(MUA_max/60)-1)):\n",
    "            if i == 0:\n",
    "                MUA_band = generated_epochs.compute_psd(fmin=100., fmax = 115., picks='all', n_jobs=n_cores)\n",
    "                avg_psd_MUA_band = np.mean(MUA_band, axis=2)\n",
    "                MUA_band_list.append(avg_psd_MUA_band)\n",
    "            elif i == int(MUA_max/60-1):\n",
    "                MUA_band = generated_epochs.compute_psd(fmin=65+i*60, fmax = MUA_max, picks='all', n_jobs=n_cores)\n",
    "                avg_psd_MUA_band = np.mean(MUA_band, axis=2)\n",
    "                MUA_band_list.append(avg_psd_MUA_band)\n",
    "            else:\n",
    "                MUA_band = generated_epochs.compute_psd(fmin = 65+i*60., fmax = 115+i*60., picks='all', n_jobs=n_cores)\n",
    "                avg_psd_MUA_band = np.mean(MUA_band, axis=2)\n",
    "                MUA_band_list.append(avg_psd_MUA_band)\n",
    "        avg_psd_MUA = np.mean(np.array(MUA_band_list), axis=0)\n",
    "        return(avg_psd_MUA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "926607c0-6a11-40df-9166-f10b0248ce01",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0b2917f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SETTINGS:\n",
    "\n",
    "# 1. Choose Asleep or Awake data\n",
    "sub_state = 'Asleep'\n",
    "\n",
    "# 2. Set number of CPU cores to use for computing PSD. Default to -1 unless another user is running an intense program\n",
    "n_cores = -1 # 1\n",
    "\n",
    "# 3. Set maximum frequency for MUA \n",
    "MUA_max = 500\n",
    "\n",
    "# 4. Set starting and ending times (in seconds)\n",
    "tmin, tmax = 0, 300\n",
    "\n",
    "# 5. Add in all subject IDs to be analyzed\n",
    "sub_ids_lst = ['846', '853', '865', '870', '871', '872', '878', '884', '888', '893']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "19228b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directories\n",
    "processed_info_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/\"\n",
    "parent_data_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/\"\n",
    "save_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/PO1 vs last day stats results/\"\n",
    "processed_roi_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/Processed Anatomical Regions Data/\"\n",
    "\n",
    "# Globals\n",
    "Fs = 2000 # Hz\n",
    "\n",
    "bmin,bmax = 0,0 # baseline start and end for z-score\n",
    "\n",
    "# TODO: change to appropriate labels for sEEG data. those below are from source space data\n",
    "roi_names = [# Right\n",
    "'rostralanteriorcingulate-rh', # Right Rostral ACC\n",
    "'caudalanteriorcingulate-rh', # Right Caudal ACC\n",
    "'postcentral-rh', # , Right S1\n",
    "'ctx-rh-insula', 'superiorfrontal-rh', # Right Insula, Right DL-PFC\n",
    "'medialorbitofrontal-rh', # Right Medial-PFC\n",
    "# Left\n",
    " 'rostralanteriorcingulate-lh', # Left Rostral ACC\n",
    " 'caudalanteriorcingulate-lh', # Left Caudal ACC\n",
    " 'postcentral-lh', # Left S1,\n",
    " 'ctx-lh-insula', 'superiorfrontal-lh', # Left Insula, Left DL-PFC,\n",
    " 'medialorbitofrontal-lh' # Left Medial-PFC\n",
    "]\n",
    "\n",
    "n_channels = len(roi_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "954b3b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of focus areas:\n",
    "# caudalanteriorcingulate\n",
    "# rostralanteriorcingulate\n",
    "# ctx-lh-insula \n",
    "# ctx-rh-insula \n",
    "# medialorbitofrontal\n",
    "# postcentral (S1)\n",
    "# superiorfrontal (PFC)\n",
    "\n",
    "# Of interest areas:\n",
    "areas_of_interest = ['caudalanteriorcingulate','rostralanteriorcingulate','ctx-lh-insula','ctx-rh-insula','medialorbitofrontal','postcentral','superiorfrontal','insula']\n",
    "\n",
    "# display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a35167-c680-46a7-b0de-8e26566ca727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0d3b04ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2341834478.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[36], line 15\u001b[0;36m\u001b[0m\n\u001b[0;31m    if str(sub_ids_lst[i]) in str(file) | not file.endswith('512-raw.fif'): | sub_state in str(file):\u001b[0m\n\u001b[0m                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df_mean_all=[]\n",
    "df_peak_all=[]\n",
    "\n",
    "#loop through to analyze each subject - Redundant for SEEG Data\n",
    "\n",
    "tmin, tmax = 0, 300  # set the timeframe of data we will use (start, end)\n",
    "MUA_max = 500 # Set the maximum frequency for MUA data\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ LOAD SUBJECT DATA @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "\n",
    "\n",
    "for i in range(0, len(sub_ids_lst)):\n",
    "    pair = [] # reset\n",
    "    for file in os.listdir(processed_info_dir):\n",
    "        if str(sub_ids_lst[i]) in str(file) | not file.endswith('512-raw.fif'): | sub_state in str(file):\n",
    "            pair.append(file)\n",
    "    \n",
    "    for z in range(0, len(pair)):\n",
    "        raw_fname = os.path.join(processed_info_dir, str(pair[z]))\n",
    "        print(f\"analyzing data for {raw_fname[:-8]}\"\n",
    "        \n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ COMPUTE PSD FROM RAW FIF @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #        \n",
    "        \n",
    "        raw = mne.io.read_raw_fif(raw_fname).crop(tmin, tmax).load_data()\n",
    "        epochs = mne.make_fixed_length_epochs(raw, duration=30, preload=False)\n",
    "        n_fft = 2048  # the FFT size (n_fft). Ideally a power of 2\n",
    "              \n",
    "        # Calculate the psds from raw data based on frequency range\n",
    "        broad1 = epochs.compute_psd(fmin=1., fmax=55., n_jobs=-1) \n",
    "        broad2 = epochs.compute_psd(fmin=65., fmax=MUA_max., n_jobs=-1) # avoid the notch filter\n",
    "        delta = epochs.compute_psd(fmin=1., fmax=4., n_jobs=-1)\n",
    "        theta = epochs.compute_psd(fmin=4., fmax=8., n_jobs=-1)\n",
    "        alpha = epochs.compute_psd(fmin=8., fmax=13., n_jobs=-1)\n",
    "        beta = epochs.compute_psd(fmin=13., fmax=30., n_jobs=-1)\n",
    "        low_gamma = epochs.compute_psd(fmin=30., fmax=55., n_jobs=-1)\n",
    "        high_gamma = epochs.compute_psd(fmin=65., fmax=100., n_jobs=-1)\n",
    "\n",
    "        # Get average power across bands (shape = epochs x channels)\n",
    "        avg_psd_broad1 = np.mean(broad1, axis=2)\n",
    "        avg_psd_broad2 = np.mean(broad1, axis=2)\n",
    "        avg_psd_delta = np.mean(delta, axis=2)\n",
    "        avg_psd_theta = np.mean(theta, axis=2)\n",
    "        avg_psd_alpha = np.mean(alpha, axis=2)\n",
    "        avg_psd_beta = np.mean(beta, axis=2)\n",
    "        avg_psd_low_gamma = np.mean(low_gamma, axis=2)\n",
    "        avg_psd_high_gamma = np.mean(high_gamma, axis=2)\n",
    "        avg_psd_MUA = np.mean(MUA, axis=2)\n",
    "              \n",
    "        # Combine broad spectrum powers and take the trace\n",
    "        avg_psd_broad = np.mean( np.array([avg_psd_broad1, avg_psd_broad2 ]), axis=0 )\n",
    "        trace_avg_psd_broad = np.trace(avg_psd_broad)\n",
    "        print(f\"Trace of the average broad spectrum power: {trace_avg_psd_broad}\")    \n",
    "        \n",
    "        # Get band PSD's normalized by the trace of the broad spectrum and convert to dB\n",
    "        avg_psd_delta = 10 * np.log10(avg_psd_delta / trace_avg_psd_broad)\n",
    "        avg_psd_theta = 10 * np.log10(avg_psd_theta / trace_avg_psd_broad)\n",
    "        avg_psd_alpha = 10 * np.log10(avg_psd_alpha / trace_avg_psd_broad)\n",
    "        avg_psd_beta = 10 * np.log10(avg_psd_beta / trace_avg_psd_broad)\n",
    "        avg_psd_low_gamma = 10 * np.log10(avg_psd_low_gamma / trace_avg_psd_broad)\n",
    "        avg_psd_high_gamma = 10 * np.log10(avg_psd_high_gamma / trace_avg_psd_broad)\n",
    "        avg_psd_MUA = 10 * np.log10(avg_psd_MUA / trace_avg_psd_broad)\n",
    "        \n",
    "        avg_psd_arr = np.array((avg_psd_delta, avg_psd_theta, \n",
    "                                avg_psd_alpha, avg_psd_beta,\n",
    "                                avg_psd_low_gamma, avg_psd_high_gamma))       \n",
    "\n",
    "              \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabbd671-f28e-4544-804d-ec8f36d0b7b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin, tmax = 0, 300  # set the timeframe of data we will use (start, end)\n",
    "MUA_max = 500 # Set the maximum frequency for MUA data\n",
    "\n",
    "for file in os.listdir(processed_info_dir):\n",
    "    if str(sub_ids_lst[i]) in str(file) | not file.endswith('512-raw.fif'): | sub_state in str(file):\n",
    "        pair.append(file)\n",
    "\n",
    "for z in range(0, len(pair)):\n",
    "    raw_fname = os.path.join(processed_info_dir, str(pair[z]))\n",
    "    print(f\"analyzing data for {raw_fname[:-8]}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7beff2b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/NY846_Day 1_Asleep1-raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 617087 =      0.000 ...   301.312 secs\n",
      "Ready.\n",
      "Reading 0 ... 617087  =      0.000 ...   301.312 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 13517 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  48 tasks      | elapsed:    2.5s\n",
      "[Parallel(n_jobs=-1)]: Done 138 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 191 out of 191 | elapsed:    4.4s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "10 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Using data from preloaded Raw for 10 events and 61440 original time points ...\n",
      "0 bad epochs dropped\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "Trace of the average broad spectrum power: 1.226713760138527e-06\n",
      "[[5.30019308e-08 6.21690744e-08 1.36135322e-07 ... 2.34543619e-07\n",
      "  3.03328733e-07 3.08639655e-07]\n",
      " [6.21514661e-08 7.89993497e-08 1.07235734e-07 ... 2.43004641e-07\n",
      "  3.17468417e-07 3.13492848e-07]\n",
      " [3.69104362e-08 4.74824828e-08 6.60911078e-08 ... 2.33654739e-07\n",
      "  2.92350022e-07 2.98471559e-07]\n",
      " ...\n",
      " [7.33767523e-08 9.28783685e-08 1.24821246e-07 ... 2.35316602e-07\n",
      "  3.11876623e-07 3.00660742e-07]\n",
      " [4.70983780e-08 1.42503655e-07 1.39564282e-07 ... 2.40598029e-07\n",
      "  3.02941279e-07 2.94068386e-07]\n",
      " [8.35529814e-08 9.03972176e-08 1.12304045e-07 ... 2.47400443e-07\n",
      "  3.05378890e-07 3.26039022e-07]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n# Get band PSD's normalized by the trace of the broad spectrum and convert to dB\\navg_psd_delta = 10 * np.log10(avg_psd_delta / trace_avg_psd_broad)\\navg_psd_theta = 10 * np.log10(avg_psd_theta / trace_avg_psd_broad)\\navg_psd_alpha = 10 * np.log10(avg_psd_alpha / trace_avg_psd_broad)\\navg_psd_beta = 10 * np.log10(avg_psd_beta / trace_avg_psd_broad)\\navg_psd_low_gamma = 10 * np.log10(avg_psd_low_gamma / trace_avg_psd_broad)\\navg_psd_high_gamma = 10 * np.log10(avg_psd_high_gamma / trace_avg_psd_broad)\\navg_psd_MUA = 10 * np.log10(avg_psd_MUA / trace_avg_psd_broad)\\n\\navg_psd_arr = np.array((avg_psd_delta, avg_psd_theta, \\n                        avg_psd_alpha, avg_psd_beta,\\n                        avg_psd_low_gamma, avg_psd_high_gamma, avg_psd_MUA))\\n\\nprint(avg_psd_arr.shape)\""
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MUA_max = 500\n",
    "raw = mne.io.read_raw_fif('../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/NY846_Day 1_Asleep1-raw.fif', preload=True)\n",
    "raw = raw.notch_filter(np.arange(60, 481, 60), n_jobs=-1, fir_design='firwin')\n",
    "epochs = mne.make_fixed_length_epochs(raw, duration=30, preload=True)\n",
    "n_fft = 2048  # the FFT size (n_fft). Ideally a power of 2\n",
    "\n",
    "\n",
    "# Calculate the psds from raw data based on frequency range\n",
    "broad1 = epochs.compute_psd(method = 'multitaper', fmin=1., fmax=55., n_jobs=-1) \n",
    "broad2 = epochs.compute_psd(method = 'multitaper', fmin=65., fmax=MUA_max, n_jobs=-1) \n",
    "delta = epochs.compute_psd(method = 'multitaper', fmin=1., fmax=4., n_jobs=-1)\n",
    "theta = epochs.compute_psd(method = 'multitaper', fmin=4., fmax=8., n_jobs=-1)\n",
    "alpha = epochs.compute_psd(method = 'multitaper', fmin=8., fmax=13., n_jobs=-1)\n",
    "beta = epochs.compute_psd(method = 'multitaper', fmin=13., fmax=30., n_jobs=-1)\n",
    "low_gamma = epochs.compute_psd(method = 'multitaper', fmin=30., fmax=55., n_jobs=-1)\n",
    "high_gamma = epochs.compute_psd(method = 'multitaper', fmin=65., fmax=100., n_jobs=-1)\n",
    "MUA = epochs.compute_psd(method = 'multitaper', fmin=100., fmax=MUA_max, n_jobs=-1)\n",
    "\n",
    "# Get average power across bands (shape = epochs x channels)\n",
    "avg_psd_broad1 = np.mean(broad1, axis=2)\n",
    "avg_psd_broad2 = np.mean(broad2, axis=2)\n",
    "avg_psd_delta = np.mean(delta, axis=2)\n",
    "avg_psd_theta = np.mean(theta, axis=2)\n",
    "avg_psd_alpha = np.mean(alpha, axis=2)\n",
    "avg_psd_beta = np.mean(beta, axis=2)\n",
    "avg_psd_low_gamma = np.mean(low_gamma, axis=2)\n",
    "avg_psd_high_gamma = np.mean(high_gamma, axis=2)\n",
    "avg_psd_MUA = np.mean(MUA, axis=2)\n",
    "\n",
    "# Combine broad spectrum powers and take the trace\n",
    "avg_psd_broad = np.mean( np.array([avg_psd_broad1, avg_psd_broad2 ]), axis=0 )\n",
    "trace_avg_psd_broad = np.trace(avg_psd_broad)\n",
    "print(f\"Trace of the average broad spectrum power: {trace_avg_psd_broad}\")  \n",
    "print(avg_psd_broad)\n",
    "\"\"\"\n",
    "# Get band PSD's normalized by the trace of the broad spectrum and convert to dB\n",
    "avg_psd_delta = 10 * np.log10(avg_psd_delta / trace_avg_psd_broad)\n",
    "avg_psd_theta = 10 * np.log10(avg_psd_theta / trace_avg_psd_broad)\n",
    "avg_psd_alpha = 10 * np.log10(avg_psd_alpha / trace_avg_psd_broad)\n",
    "avg_psd_beta = 10 * np.log10(avg_psd_beta / trace_avg_psd_broad)\n",
    "avg_psd_low_gamma = 10 * np.log10(avg_psd_low_gamma / trace_avg_psd_broad)\n",
    "avg_psd_high_gamma = 10 * np.log10(avg_psd_high_gamma / trace_avg_psd_broad)\n",
    "avg_psd_MUA = 10 * np.log10(avg_psd_MUA / trace_avg_psd_broad)\n",
    "\n",
    "avg_psd_arr = np.array((avg_psd_delta, avg_psd_theta, \n",
    "                        avg_psd_alpha, avg_psd_beta,\n",
    "                        avg_psd_low_gamma, avg_psd_high_gamma, avg_psd_MUA))\n",
    "\n",
    "print(avg_psd_arr.shape)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62002139",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "    Using multitaper spectrum estimation with 7 DPSS windows\n",
      "(10, 191)\n"
     ]
    }
   ],
   "source": [
    "avg_psd_broad2 = notch_filter_analysis('broad', epochs, avg_psd_broad1)\n",
    "\n",
    "print(avg_psd_broad2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d357faee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.32991973e-08, 1.55601192e-08, 3.41731882e-08, ...,\n",
       "        1.98683879e-07, 2.56132175e-07, 2.57977240e-07],\n",
       "       [1.56070543e-08, 1.97885003e-08, 2.69599489e-08, ...,\n",
       "        2.02106939e-07, 2.57325627e-07, 2.67126102e-07],\n",
       "       [9.28429485e-09, 1.18960995e-08, 1.66466010e-08, ...,\n",
       "        1.99164967e-07, 2.45019482e-07, 2.54704363e-07],\n",
       "       ...,\n",
       "       [1.84402558e-08, 2.32840705e-08, 3.13713068e-08, ...,\n",
       "        1.98691208e-07, 2.58538839e-07, 2.56205803e-07],\n",
       "       [1.19067672e-08, 3.57222209e-08, 3.50937337e-08, ...,\n",
       "        2.01884189e-07, 2.58501425e-07, 2.54873156e-07],\n",
       "       [2.09808242e-08, 2.26545803e-08, 2.82308771e-08, ...,\n",
       "        2.06035366e-07, 2.59459621e-07, 2.70992168e-07]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_psd_broad2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35a1c9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68263df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "        data_epo=combined_data_trials\n",
    "        print(data_epo.shape)\n",
    "        data_epo_zscore=np.copy(data_epo)\n",
    "        for i in range(data_epo.shape[0]): # for each epoch\n",
    "            for j in range(data_epo.shape[1]): # for each channel\n",
    "                # compute mean and std of baseline\n",
    "                base_mean = np.mean(data_epo[i,j,:len_baseline_samples])\n",
    "                base_std = np.std(data_epo[i,j,:len_baseline_samples])\n",
    "\n",
    "                # compute z-scored data from baseline stats\n",
    "                data_epo_zscore[i,j,:] = (data_epo[i,j,:]-base_mean)/base_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a863805",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'psds_broad1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpsds_broad1\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'psds_broad1' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac97fd4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150001,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9295d303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1711115917.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(sub_fname)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "df_mean_all=[]\n",
    "df_peak_all=[]\n",
    "\n",
    "#loop through to analyze each subject - Redundant for SEEG Data\n",
    "\"\"\"for sub_folder in os.listdir(parent_data_dir):\n",
    "    sub_num='' # reset\n",
    "    # ignores hidden files\n",
    "    if sub_folder.startswith('.') or sub_folder not in chosen_list:\n",
    "        continue\n",
    "    elif sub_folder in chosen_list:\n",
    "        if not os.path.exists(os.path.join(save_dir,sub_folder)):\n",
    "            os.mkdir(os.path.join(save_dir,sub_folder))        \n",
    "        sub_num=sub_folder\n",
    "\n",
    "\n",
    "    data_dir = os.path.join(parent_data_dir,sub_num)\n",
    "\n",
    "    print(f\"{sub_num}\\nReading data...\\n\")\n",
    "\n",
    "    conditions=[]\"\"\"\n",
    "\n",
    "display.clear_output(wait=True) # Not sure\n",
    "\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ LOAD SUBJECT DATA @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "\n",
    "    \n",
    "tmin, tmax = 0, 300  # set the timeframe of data we will use (start, end)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "for file in os.listdir(processed_info_dir):\n",
    "    if file.endswith('.fif') and not file.endswith('512-raw.fif'):\n",
    "        raw_fname = os.path.join(processed_info_dir, str(file))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ LOAD PROCESSED INFO @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "    \n",
    "\n",
    "    # # Load in pain rating for each stimuli - pain ratings from Intracranial subjects are WIP\n",
    "    # pain_ratings = scio.loadmat(os.path.join(processed_info_dir,sub_num+'_pain_ratings.mat'))\n",
    "    # pain_ratings = pain_ratings['pain_ratings'].tolist()[0]\n",
    "    # print(f\"*pain_ratings.size = {len(pain_ratings)}*\\n\")\n",
    "\n",
    "    # # Load in drop log for bad trials\n",
    "    # drop_log = scio.loadmat(os.path.join(processed_info_dir,sub_num+'_drop_log.mat'))\n",
    "    # drop_log = drop_log['drop_log'] # leave as array\n",
    "    # print(f\"*drop_log.size = {drop_log.shape[0]}*\\n\")\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ COMPUTE PSD FROM RAW FIF @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "\n",
    "    # TODO: implement following tutorial for psd from raw.fif\n",
    "    # https://mne.tools/0.21/auto_examples/time_frequency/plot_compute_raw_data_spectrum.html\n",
    "    # done\n",
    "\n",
    "    \n",
    "    # Extract PSD into each frequency band of interest\n",
    "    broad1 = epochs.compute_psd(fmin=1., fmax=55., n_jobs=-1) \n",
    "    broad2 = epochs.compute_psd(fmin=65., fmax=MUA_max., n_jobs=-1) # avoid the notch filter\n",
    "    delta = epochs.compute_psd(fmin=1., fmax=4., n_jobs=-1)\n",
    "    theta = epochs.compute_psd(fmin=4., fmax=8., n_jobs=-1)\n",
    "    alpha = epochs.compute_psd(fmin=8., fmax=13., n_jobs=-1)\n",
    "    beta = epochs.compute_psd(fmin=13., fmax=30., n_jobs=-1)\n",
    "    low_gamma = epochs.compute_psd(fmin=30., fmax=55., n_jobs=-1)\n",
    "    high_gamma = epochs.compute_psd(fmin=65., fmax=100., n_jobs=-1)\n",
    "    MUA = fmin = 100, fmax = MUA_max\n",
    "    \n",
    "    # Get data (shape = epochs x channels x powers in each band (1 hz width))\n",
    "    psds_broad1 = broad1.get_data()\n",
    "    psds_broad2 = broad2.get_data()\n",
    "    psds_delta = delta.get_data()\n",
    "    psds_theta = theta.get_data()\n",
    "    psds_alpha = alpha.get_data()\n",
    "    psds_beta = beta.get_data() \n",
    "    psds_low_gamma = low_gamma.get_data()\n",
    "    psds_high_gamma = high_gamma.get_data()\n",
    "\n",
    "    # Get average power across bands (shape = epochs x channels)\n",
    "    avg_psd_broad1 = np.mean(psds_broad1, axis=2)\n",
    "    avg_psd_broad2 = np.mean(psds_broad1, axis=2)\n",
    "    avg_psd_delta = np.mean(psds_delta, axis=2)\n",
    "    avg_psd_theta = np.mean(psds_theta, axis=2)\n",
    "    avg_psd_alpha = np.mean(psds_alpha, axis=2)\n",
    "    avg_psd_beta = np.mean(psds_beta, axis=2)\n",
    "    avg_psd_low_gamma = np.mean(psds_low_gamma, axis=2)\n",
    "    avg_psd_high_gamma = np.mean(psds_high_gamma, axis=2)\n",
    "\n",
    "    # Combine broad spectrum powers and take the trace\n",
    "    avg_psd_broad = np.mean( np.array([avg_psd_broad1, avg_psd_broad2 ]), axis=0 )\n",
    "    trace_avg_psd_broad = np.trace(avg_psd_broad)\n",
    "    print(\"Trace of the average broad spectrum power: \",trace_avg_psd_broad)\n",
    "\n",
    "    # Get band PSD's normalized by the trace of the broad spectrum and convert to dB\n",
    "    avg_psd_delta = 10 * np.log10(avg_psd_delta / trace_avg_psd_broad)\n",
    "    avg_psd_theta = 10 * np.log10(avg_psd_theta / trace_avg_psd_broad)\n",
    "    avg_psd_alpha = 10 * np.log10(avg_psd_alpha / trace_avg_psd_broad)\n",
    "    avg_psd_beta = 10 * np.log10(avg_psd_beta / trace_avg_psd_broad)\n",
    "    avg_psd_low_gamma = 10 * np.log10(avg_psd_low_gamma / trace_avg_psd_broad)\n",
    "    avg_psd_high_gamma = 10 * np.log10(avg_psd_high_gamma / trace_avg_psd_broad)\n",
    "\n",
    "    avg_psd_arr = np.array((avg_psd_delta, avg_psd_theta, \n",
    "                            avg_psd_alpha, avg_psd_beta,\n",
    "                            avg_psd_low_gamma, avg_psd_high_gamma))\n",
    "    \n",
    "    # # TODO: you may or may not need to use the following code block but im leaving it here just in case \n",
    "    # data_chs_array = np.zeros((len(roi_names),len(epo_times),Fs))\n",
    "    # data_trials_tmp=np.zeros((len(epo_times),Fs))\n",
    "    # for j,raw in enumerate(raw_objects_lst): # for each roi\n",
    "    #     extracted_data_tmp=raw.data\n",
    "    #     for i in range(len(epo_times)): # for each trial\n",
    "    #         #all vertices\n",
    "    #         all_vertices_tmp=extracted_data_tmp[:,int(epo_times[i]+tmin*Fs):int(epo_times[i]+tmax*Fs)]\n",
    "    #         #averaged vertices\n",
    "    #         vertices_averaged_tmp=np.mean(all_vertices_tmp,axis=0)\n",
    "    #         #store trials x time data in numpy array\n",
    "    #         data_trials_tmp[i,:] = vertices_averaged_tmp\n",
    "    #     data_chs_array[j,...] = data_trials_tmp\n",
    "    # # data in Array format \n",
    "    # data_chs_corrected_array=np.transpose(data_chs_array,(1,0,2))\n",
    "\n",
    "    # Z-score data by for loop\n",
    "    # TODO: z-score the data on a segment of the last-day data, and exclude that segment from the last-day data to be analyzed\n",
    "    data_epo=combined_data_trials\n",
    "    print(data_epo.shape)\n",
    "    data_epo_zscore=np.copy(data_epo)\n",
    "    for i in range(data_epo.shape[0]): # for each epoch\n",
    "        for j in range(data_epo.shape[1]): # for each channel\n",
    "            # compute mean and std of baseline\n",
    "            base_mean = np.mean(data_epo[i,j,:len_baseline_samples])\n",
    "            base_std = np.std(data_epo[i,j,:len_baseline_samples])\n",
    "        \n",
    "            # compute z-scored data from baseline stats\n",
    "            data_epo_zscore[i,j,:] = (data_epo[i,j,:]-base_mean)/base_std\n",
    "\n",
    "    # compare data before and after z-score\n",
    "    print(data_epo[0,0,len_baseline_samples:len_baseline_samples+10],'\\n')\n",
    "    print(data_epo_zscore[0,0,len_baseline_samples:len_baseline_samples+10])\n",
    "\n",
    "    # TODO: epochs object will just have z-scored day 1 data and last day data\n",
    "    # Create info object required for epochs object\n",
    "    info = mne.create_info(roi_names, sfreq=Fs,ch_types='eeg')\n",
    "    data_epo = data_chs_corrected_array\n",
    "    # Create events array for Epochs object\n",
    "    # TODO: 0*i means the start time from each piece of data will just be 0\n",
    "    # because we dont have any epochs in the data, we are treating the entire thing\n",
    "    # as one epoch.\n",
    "    # NOTE: the i in the last term of the list should just populate 0 and then 1,\n",
    "    # indicating day 1 as label of 0 and last day as label of 1.\n",
    "    \n",
    "    events=np.array([[0*i,0,i] for i in range(len(conditions))])\n",
    "\n",
    "    zepochs = mne.EpochsArray(data=data_epo_zscore,\n",
    "                              info=info,\n",
    "                              tmin=tmin,\n",
    "                              events=events,\n",
    "                              event_id=event_ids_dict,\n",
    "                              baseline=(None,bmax),\n",
    "                              )\n",
    "    print(zepochs)\n",
    "    epochs=zepochs \n",
    "    del zepochs\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ CONSTRUCT TFR OBJECT FOR DF @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "    \n",
    "    # TODO: do not implement ERDS, leaving this code here to figure out how to use it to set up the dataframe\n",
    "    tfr = tfr_multitaper(\n",
    "        epochs,\n",
    "        freqs=freqs,\n",
    "        n_cycles=freqs,\n",
    "        use_fft=True,\n",
    "        return_itc=False,\n",
    "        average=False,\n",
    "        decim=2,\n",
    "    )\n",
    "    tfr.crop(tmin, tmax).apply_baseline(baseline, mode=\"percent\")\n",
    "\n",
    "    for event in event_ids:\n",
    "        # select desired epochs for visualization\n",
    "        tfr_ev = tfr[event]\n",
    "        fig, axes = plt.subplots(\n",
    "            1, 4, figsize=(12, 4), gridspec_kw={\"width_ratios\": [10, 10, 10, 1]}\n",
    "        )\n",
    "        for ch, ax in enumerate(axes[:-1]):  # for each channel\n",
    "            # positive clusters\n",
    "            _, c1, p1, _ = pcluster_test(tfr_ev.data[:, ch], tail=1, **kwargs)\n",
    "            # negative clusters\n",
    "            _, c2, p2, _ = pcluster_test(tfr_ev.data[:, ch], tail=-1, **kwargs)\n",
    "\n",
    "            # note that we keep clusters with p <= 0.05 from the combined clusters\n",
    "            # of two independent tests; in this example, we do not correct for\n",
    "            # these two comparisons\n",
    "            c = np.stack(c1 + c2, axis=2)  # combined clusters\n",
    "            p = np.concatenate((p1, p2))  # combined p-values\n",
    "            mask = c[..., p <= 0.05].any(axis=-1)\n",
    "\n",
    "            # plot TFR (ERDS map with masking)\n",
    "            tfr_ev.average().plot(\n",
    "                [ch],\n",
    "                cmap=\"RdBu\",\n",
    "                cnorm=cnorm,\n",
    "                axes=ax,\n",
    "                colorbar=False,\n",
    "                show=False,\n",
    "                mask=mask,\n",
    "                mask_style=\"mask\",\n",
    "            )\n",
    "\n",
    "            ax.set_title(epochs.ch_names[ch], fontsize=10)\n",
    "            ax.axvline(0, linewidth=1, color=\"black\", linestyle=\":\")  # event\n",
    "            if ch != 0:\n",
    "                ax.set_ylabel(\"\")\n",
    "                ax.set_yticklabels(\"\")\n",
    "        fig.colorbar(axes[0].images[-1], cax=axes[-1]).ax.set_yscale(\"linear\")\n",
    "        fig.suptitle(f\"ERDS ({event})\")\n",
    "        plt.show()\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ SET UP PANDAS DATAFRAMES @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "\n",
    "    #Export tfr to pandas df in seconds\n",
    "    df = tfr.to_data_frame(time_format=None, long_format=False)\n",
    "    df.head()\n",
    "\n",
    "    #Plot with confidence bands \n",
    "    df = tfr.to_data_frame(time_format=None, long_format=True)\n",
    "\n",
    "    # Map to frequency bands:\n",
    "    freq_bounds = {'_': 0,\n",
    "                   'delta': 4,\n",
    "                   'theta': 7,\n",
    "                   'alpha': 13,\n",
    "                   'beta': 35,\n",
    "                   'low-gamma': 55,\n",
    "                   'notch': 65,\n",
    "                   'high-gamma': 100,\n",
    "                   'MUA': 500\n",
    "                  }\n",
    "\n",
    "    df['band'] = pd.cut(df['freq'], list(freq_bounds.values()),\n",
    "                        labels=list(freq_bounds)[1:])\n",
    "\n",
    "    # Filter to retain only relevant frequency bands:\n",
    "    freq_bands_of_interest = [\n",
    "                              # 'delta', \n",
    "                              'theta', \n",
    "                              'alpha', \n",
    "                              'beta', \n",
    "                              'low-gamma', \n",
    "                              'high-gamma',\n",
    "                              'MUA',\n",
    "        ]\n",
    "    df = df[df.band.isin(freq_bands_of_interest)]\n",
    "    df['band'] = df['band'].cat.remove_unused_categories()\n",
    "\n",
    "    # Order channels for plotting:\n",
    "    df['channel'] = df['channel'].cat.reorder_categories(tuple(roi_names),\n",
    "                                                         ordered=True)\n",
    "\n",
    "    g = sns.FacetGrid(df, row='band', col='channel', margin_titles=True)\n",
    "    g.map(sns.lineplot, 'time', 'value', n_boot=10)\n",
    "    axline_kw = dict(color='black', linestyle='dashed', linewidth=0.5, alpha=0.5)\n",
    "    g.map(plt.axhline, y=0, **axline_kw)\n",
    "    g.map(plt.axvline, x=0, **axline_kw)\n",
    "    g.set(xlim=(tmin+0.5,tmax-0.5))\n",
    "    # g.set(ylim=(None, 3))\n",
    "    g.set_axis_labels(\"Time (s)\", \"Z-scored Band Power (uV^2)\")\n",
    "    g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
    "    g.add_legend(ncol=2, loc='lower center')\n",
    "    g.fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.08)\n",
    "    plt.show();\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    # Plot the ERDS based on average, peak amplitude, and area-under-the-curve (AUC), one by one.\n",
    "    df_mean = (df.query(f\"time > {tmin}\")\n",
    "                 .groupby(['epoch','band','channel'])[['value']]\n",
    "                 .mean()\n",
    "                 .dropna(subset=['value']) # needed to remove taking average over non-numeric values\n",
    "                 .reset_index())\n",
    "\n",
    "    df_mean_all.append(df_mean) #when it loops through each subject, will save the mean in the array\n",
    "\n",
    "    df_peak = (df.query(f\"time > {tmin} & time < {tmax}\")\n",
    "                 .groupby(['epoch','band','channel'])[['value']]\n",
    "                 .max() # peak amplitude\n",
    "                 .dropna(subset=['value']) # needed to remove taking average over non-numeric values\n",
    "                 .reset_index())\n",
    "    df_peak_all.append(df_peak) #when it loops through each subject, will save the peak\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ EVALUATE AND SAVE STATS OUTPUTS @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "    stats_plot(df_mean, save_dir, 'mean')\n",
    "    stats_plot(df_peak, save_dir, 'peak')\n",
    "    \n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794755a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae315a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42048b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegenv",
   "language": "python",
   "name": "eegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
