{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14889b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import scipy.io as scio\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mne.time_frequency import tfr_multitaper\n",
    "from mne.stats import permutation_cluster_1samp_test as pcluster_test\n",
    "import scipy.stats as stats\n",
    "from statannotations.Annotator import Annotator\n",
    "%matplotlib inline\n",
    "# sns.set(style=\"whitegrid\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b18f6c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "501d8e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_multiple_elements(list_object, indices):\n",
    "    indices = sorted(indices, reverse=True)\n",
    "    for idx in indices:\n",
    "        if idx < len(list_object):\n",
    "            list_object.pop(idx) # define functions for extracting relevant epochs\n",
    "            \n",
    "def stats_plot(df, save_dir, feature, file_name):\n",
    "    '''\n",
    "    df = dataframe (mean, peak)\n",
    "    save_dir = directory to save stats PNG and TXT files\n",
    "    feature = 'mean','peak'\n",
    "    '''    \n",
    "\n",
    "    pairs=[((f,conditions[0]),(f,conditions[1])) for f in freq_bands_of_interest]\n",
    "\n",
    "    for roi in roi_names:        \n",
    "        df_ch_tmp=df.query(f\"channel=='{roi}'\")\n",
    "       \n",
    "\n",
    "        #stats plot\n",
    "        plt.figure(figsize=(8.6,6.5));\n",
    "        ax_tmp = sns.boxplot(data=df_ch_tmp,x='band', y='value', hue='condition',\n",
    "                palette='pastel', order=freq_bands_of_interest,\n",
    "                hue_order=conditions,\n",
    "                linewidth=0.5)\n",
    "\n",
    "        ax_tmp.set_title(f\"{roi} | Resting state Power | Feature: '{feature}' \",pad=30)\n",
    "\n",
    "        #stats output\n",
    "        x='band'\n",
    "        y='value'\n",
    "        hue='condition'\n",
    "        order=freq_bands_of_interest\n",
    "        hue_order = conditions\n",
    "\n",
    "        test = 'Mann-Whitney'\n",
    "        print(f\"******************************************************{roi} {test}******************************************************\")\n",
    "        annot = Annotator(ax_tmp, pairs, data=df_ch_tmp, x=x, y=y, hue=hue, order=order,hue_order=hue_order)\n",
    "        annot.configure(test=test, text_format='star', loc='outside',comparisons_correction=None)\n",
    "        annot.apply_test()\n",
    "        annot.annotate()\n",
    "\n",
    "        # save plot and tfr data\n",
    "        # TODO: follow same naming convention as in preprocessing code\n",
    "        # Add region to end, removed raw.fif\n",
    "        save_fname = f\"{file_name[:-4]}_{roi}\"\n",
    "        \n",
    "        # plot as png\n",
    "        plt.savefig(os.path.join(save_dir,sub_folder,save_fname+\".png\"))\n",
    "        plt.figure()\n",
    "        \n",
    "        # tfr data as csv\n",
    "        df_ch_tmp.to_csv(os.path.join(save_dir,sub_folder,save_fname+\".csv\"))\n",
    "        display.clear_output(wait=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "add7670f-a0da-4cd1-acbb-4da989b6ec21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File pair generator\n",
    "\n",
    "def file_pair_generator(file_dir, sub_id, sub_state):\n",
    "    same_state_double_pairs = []\n",
    "    day_1_data = []\n",
    "    day_last_data = []\n",
    "    for file in os.listdir(file_dir):\n",
    "        if str(sub_id) in file and str(sub_state) in file and not '512' in file:\n",
    "            same_state_double_pairs.append(file)\n",
    "    for i in range(0, len(same_state_double_pairs)):\n",
    "        if 'Day 1' in same_state_double_pairs[i]:\n",
    "            day_1_data.append(same_state_double_pairs[i])\n",
    "        else:\n",
    "            day_last_data.append(same_state_double_pairs[i])\n",
    "    return day_1_data, day_last_data\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d5f6060b-2c9f-4fb3-ba2f-b6815a066be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions are divided into surface and depth generators, surface generator allows users to select chosen nodes given image data\n",
    "# depth generator used for when images can offer no better info on channel location\n",
    "\n",
    "def ROI_Surface_Generator(ROI_Data_Path, ROI_sub_id, surface_side): # surface side must be 'lh' (lefthand) or 'rh' (righthand)\n",
    "    for file in os.listdir(ROI_Data_Path):\n",
    "        if str(ROI_sub_id) in file and not str('depth') in file and str(surface_side) in file:\n",
    "            processed_roi_csv = pd.read_csv(os.path.join(ROI_Data_Path, file))\n",
    "\n",
    "    ROI_channels = processed_roi_csv['ID'].to_list()\n",
    "    Chosen_ROI_Channels = [] # reset\n",
    "    channel_ids = 0\n",
    "    \n",
    "    while channel_ids != len(ROI_channels):\n",
    "        verification_status = input(f\"Proposed Channel in Region of Interest: {ROI_channels[channel_ids]} - Verify from Image, Keep y/n ?\")\n",
    "        if verification_status == 'y':\n",
    "            Chosen_ROI_Channels.append(ROI_channels[channel_ids])\n",
    "            channel_ids = channel_ids + 1\n",
    "            display.clear_output(wait=True)\n",
    "        elif verification_status == 'n':\n",
    "            channel_ids = channel_ids + 1\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            print(\"Invalid Reponse, please enter 'y' or 'n'\")\n",
    "            channel_ids = channel_ids + 0\n",
    "            display.clear_output(wait=True)\n",
    "            \n",
    "    return(Chosen_ROI_Channels) # returns chosen channels as list of channel IDs\n",
    "\n",
    "\n",
    "\n",
    "def ROI_Depth_Generator(ROI_Data_Path, ROI_sub_id): # use for files containing 'depth'\n",
    "    for file in os.listdir(ROI_Data_Path):\n",
    "        if str(ROI_sub_id) in file and str('depth') in file:\n",
    "            processed_roi_csv = pd.read_csv(os.path.join(ROI_Data_Path, file))\n",
    "    \n",
    "    ROI_channels = processed_roi_csv['ID'].to_list()\n",
    "    return(ROI_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "78762c37-4bdf-443c-bf89-0b0b3dbde607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automate the generation of broad or MUA (>100 Hz) psds and combine them\n",
    "\n",
    "def notch_filter_analysis(broad_or_MUA, generated_epochs, avg_psd_broad_lower, picks): # enter 'broad' or 'MUA' for broad_or_MUA argument\n",
    "    if broad_or_MUA == 'broad':\n",
    "        broad_band_list = [] # reset\n",
    "        for i in range(0, (int(MUA_max/60)-1)):\n",
    "            if i == 0:\n",
    "                broad_band = generated_epochs.compute_psd(fmin=65., fmax=117., picks=picks, n_jobs=n_cores)\n",
    "                avg_psd_broad_band = np.mean(broad_band, axis=2)\n",
    "                broad_band_list.append(avg_psd_broad_band)\n",
    "            elif i == (int(MUA_max/60)-1):\n",
    "                broad_band = generated_epochs.compute_psd(fmin=63+i*60., fmax = MUA_max, picks = picks, n_jobs=n_cores)\n",
    "                avg_psd_broad_band = np.mean(broad_band, axis=2)\n",
    "                broad_band_list.append(avg_psd_broad_band)\n",
    "            else:\n",
    "                broad_band = generated_epochs.compute_psd(fmin=63+i*60., fmax=57+(i+1)*60., picks=picks, n_jobs=n_cores)\n",
    "                avg_psd_broad_band = np.mean(broad_band, axis=2)\n",
    "                broad_band_list.append(avg_psd_broad_band)\n",
    "        broad_band_list.append(avg_psd_broad_lower)\n",
    "        avg_psd_broad = np.mean(np.array(broad_band_list), axis=0)\n",
    "        return(avg_psd_broad)\n",
    "    elif broad_or_MUA == 'MUA':\n",
    "        MUA_band_list = [] # reset\n",
    "        for i in range(0, (int(MUA_max/60)-1)):\n",
    "            if i == 0:\n",
    "                MUA_band = generated_epochs.compute_psd(fmin=100., fmax = 117., picks=picks, n_jobs=n_cores)\n",
    "                avg_psd_MUA_band = np.mean(MUA_band, axis=2)\n",
    "                MUA_band_list.append(avg_psd_MUA_band)\n",
    "            elif i == int(MUA_max/60-1):\n",
    "                MUA_band = generated_epochs.compute_psd(fmin=63+i*60, fmax = MUA_max, picks=picks, n_jobs=n_cores)\n",
    "                avg_psd_MUA_band = np.mean(MUA_band, axis=2)\n",
    "                MUA_band_list.append(avg_psd_MUA_band)\n",
    "            else:\n",
    "                MUA_band = generated_epochs.compute_psd(fmin = 63+i*60., fmax = 117+i*60., picks=picks, n_jobs=n_cores)\n",
    "                avg_psd_MUA_band = np.mean(MUA_band, axis=2)\n",
    "                MUA_band_list.append(avg_psd_MUA_band)\n",
    "        avg_psd_MUA = np.mean(np.array(MUA_band_list), axis=0)\n",
    "        return(avg_psd_MUA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "926607c0-6a11-40df-9166-f10b0248ce01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def epoch_generator(fif_file_path, file_pair, picks):\n",
    "    avg_psd_arr_lst = [] # reset\n",
    "    for i in range(0, len(file_pair)):\n",
    "        raw = mne.io.read_raw_fif(os.path.join(fif_file_path, str(file_pair[i]))).crop(tmin, tmax).load_data()\n",
    "        print(f\"processing data for {file_pair[i]}\")\n",
    "        raw = raw.notch_filter(np.arange(120, 481, 60), n_jobs=-1, fir_design='firwin', notch_widths = 6) # inital notch filter for all harmonics\n",
    "        epochs = mne.make_fixed_length_epochs(raw, duration=30, preload=False)\n",
    "        n_fft = 2048  # the FFT size (n_fft). Ideally a power of 2\n",
    "\n",
    "    # Calculate the psds from raw data based on frequency range\n",
    "        broad1 = epochs.compute_psd(fmin=1., fmax=55., picks=picks, n_jobs=n_cores)\n",
    "        delta = epochs.compute_psd(fmin=1., fmax=4., picks=picks, n_jobs=n_cores)\n",
    "        theta = epochs.compute_psd(fmin=4., fmax=8., picks=picks, n_jobs=n_cores)\n",
    "        alpha = epochs.compute_psd(fmin=8., fmax=13., picks=picks, n_jobs=n_cores)\n",
    "        beta = epochs.compute_psd(fmin=13., fmax=30., picks=picks, n_jobs=n_cores)\n",
    "        low_gamma = epochs.compute_psd(fmin=30., fmax=55., picks=picks, n_jobs=n_cores)\n",
    "        high_gamma = epochs.compute_psd(fmin=65., fmax=100., picks=picks, n_jobs=n_cores)\n",
    "    \n",
    "    \n",
    "    # Get average power across bands (shape = epochs x channels)\n",
    "        avg_psd_broad1 = np.mean(broad1, axis=2)\n",
    "        avg_psd_delta = np.mean(delta, axis=2)\n",
    "        avg_psd_theta = np.mean(theta, axis=2)\n",
    "        avg_psd_alpha = np.mean(alpha, axis=2)\n",
    "        avg_psd_beta = np.mean(beta, axis=2)\n",
    "        avg_psd_low_gamma = np.mean(low_gamma, axis=2)\n",
    "        avg_psd_high_gamma = np.mean(high_gamma, axis=2)\n",
    "        \n",
    "    # Calculate the average MUA and larger broad frequencies using the notch filter analysis function\n",
    "        avg_psd_broad = notch_filter_analysis('broad', epochs, avg_psd_broad1, picks)\n",
    "        avg_psd_MUA = notch_filter_analysis('MUA', epochs, avg_psd_broad1, picks) # Final argument is neccecary for funtion, but plays no part in MUA calculations\n",
    "        display.clear_output()\n",
    "        print(f\"applying notch filter across all epochs for broad2 analysis and MUA frequencies\")\n",
    "\n",
    "\n",
    "    # Combine broad spectrum powers and take the trace\n",
    "        trace_avg_psd_broad = np.trace(avg_psd_broad)\n",
    "        display.clear_output()\n",
    "        print(f\"Trace of the average broad spectrum power: {trace_avg_psd_broad}\")    \n",
    "\n",
    "    # Get band PSD's normalized by the trace of the broad spectrum and convert to dB\n",
    "        avg_psd_delta = 10 * np.log10(avg_psd_delta / trace_avg_psd_broad)\n",
    "        avg_psd_theta = 10 * np.log10(avg_psd_theta / trace_avg_psd_broad)\n",
    "        avg_psd_alpha = 10 * np.log10(avg_psd_alpha / trace_avg_psd_broad)\n",
    "        avg_psd_beta = 10 * np.log10(avg_psd_beta / trace_avg_psd_broad)\n",
    "        avg_psd_low_gamma = 10 * np.log10(avg_psd_low_gamma / trace_avg_psd_broad)\n",
    "        avg_psd_high_gamma = 10 * np.log10(avg_psd_high_gamma / trace_avg_psd_broad)\n",
    "        avg_psd_MUA = 10 * np.log10(avg_psd_MUA / trace_avg_psd_broad)\n",
    "\n",
    "        avg_psd_arr = np.array((avg_psd_delta, avg_psd_theta, \n",
    "                            avg_psd_alpha, avg_psd_beta,\n",
    "                            avg_psd_low_gamma, avg_psd_high_gamma, avg_psd_MUA)) \n",
    "\n",
    "        avg_psd_arr_lst.append(avg_psd_arr)\n",
    "\n",
    "    ID_sub_state_psd_arr = np.concatenate(avg_psd_arr_lst, axis = 0)\n",
    "    return(ID_sub_state_psd_arr)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5884e3-07d6-4dfc-b069-45f1e65ddd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0b2917f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SETTINGS:\n",
    "\n",
    "# 1. Choose Asleep or Awake data\n",
    "sub_state = 'Asleep'\n",
    "\n",
    "# 2. Set number of CPU cores to use for computing PSD. Default to -1 unless another user is running an intense program\n",
    "n_cores = -1 # 1\n",
    "\n",
    "# 3. Set maximum frequency for MUA \n",
    "MUA_max = 500\n",
    "\n",
    "# 4. Set starting and ending times (in seconds)\n",
    "tmin, tmax = 0, 300\n",
    "\n",
    "# 5. Add in all subject IDs to be analyzed\n",
    "sub_ids_lst = ['846', '853', '865', '870', '871', '872', '878', '884', '888', '893']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "19228b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directories\n",
    "processed_info_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/\"\n",
    "parent_data_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/\"\n",
    "save_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/PO1 vs last day stats results/\"\n",
    "processed_roi_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/Processed Anatomical Regions Data/\"\n",
    "# Globals\n",
    "Fs = 2000 # Hz\n",
    "\n",
    "bmin,bmax = 0,0 # baseline start and end for z-score\n",
    "\n",
    "# TODO: change to appropriate labels for sEEG data. those below are from source space data\n",
    "roi_names = [# Right\n",
    "'rostralanteriorcingulate-rh', # Right Rostral ACC\n",
    "'caudalanteriorcingulate-rh', # Right Caudal ACC\n",
    "'postcentral-rh', # , Right S1\n",
    "'ctx-rh-insula', 'superiorfrontal-rh', # Right Insula, Right DL-PFC\n",
    "'medialorbitofrontal-rh', # Right Medial-PFC\n",
    "# Left\n",
    " 'rostralanteriorcingulate-lh', # Left Rostral ACC\n",
    " 'caudalanteriorcingulate-lh', # Left Caudal ACC\n",
    " 'postcentral-lh', # Left S1,\n",
    " 'ctx-lh-insula', 'superiorfrontal-lh', # Left Insula, Left DL-PFC,\n",
    " 'medialorbitofrontal-lh' # Left Medial-PFC\n",
    "]\n",
    "\n",
    "n_channels = len(roi_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "954b3b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of focus areas:\n",
    "# caudalanteriorcingulate\n",
    "# rostralanteriorcingulate\n",
    "# ctx-lh-insula \n",
    "# ctx-rh-insula \n",
    "# medialorbitofrontal\n",
    "# postcentral (S1)\n",
    "# superiorfrontal (PFC)\n",
    "\n",
    "# Of interest areas:\n",
    "areas_of_interest = ['caudalanteriorcingulate','rostralanteriorcingulate','ctx-lh-insula','ctx-rh-insula','medialorbitofrontal','postcentral','superiorfrontal','insula']\n",
    "\n",
    "# display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "70a35167-c680-46a7-b0de-8e26566ca727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_day_data, last_day_data = file_pair_generator(processed_info_dir, sub_ids_lst[0], sub_state)\n",
    "\n",
    "\n",
    "selected_picks = ROI_Depth_Generator(processed_roi_dir, sub_ids_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "099719fc-e63a-451b-813a-c5b8d3bdde88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(selected_picks)):\n",
    "    if selected_picks[i][-2:][:-1] == '0':\n",
    "        selected_picks[i] = selected_picks[i][:-2] + selected_picks[i][-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b94f93ea-be10-4001-bd94-1c974344f8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DLI5',\n",
       " 'DLI6',\n",
       " 'DLI7',\n",
       " 'DLI8',\n",
       " 'DLI9',\n",
       " 'DRI1',\n",
       " 'DRI2',\n",
       " 'DRI3',\n",
       " 'DRI4',\n",
       " 'DRI5',\n",
       " 'DRI6',\n",
       " 'DRI7',\n",
       " 'DRI8']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "2639650a-91d0-4343-8882-bf5bc4e3f148",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trace of the average broad spectrum power: 2.903425428381571e-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[  5.25780296,   7.54806215,   9.9934486 , ...,   6.3871505 ,\n",
       "           8.7088538 ,   9.30337998],\n",
       "        [  4.49932452,   6.79739526,   8.5212734 , ...,   5.36173778,\n",
       "           7.30015704,   7.60447122],\n",
       "        [  4.79037236,   8.30781135,   8.37968856, ...,   4.37807532,\n",
       "           5.90175496,   5.8807656 ],\n",
       "        ...,\n",
       "        [  4.63616623,   7.29502711,   9.1276783 , ...,   6.7783125 ,\n",
       "           8.99041388,   9.70718307],\n",
       "        [  6.78589157,   8.58410892,   9.75041991, ...,   7.82454167,\n",
       "           9.80262298,  10.28889301],\n",
       "        [  4.16684027,   5.80140389,   7.45584053, ...,   4.78436474,\n",
       "           5.88334518,   6.40422303]],\n",
       "\n",
       "       [[  2.45392735,   3.40846897,   4.97625038, ...,   2.73567058,\n",
       "           6.05665928,   7.21767279],\n",
       "        [  2.79907713,   3.80892404,   5.51779345, ...,   2.25895794,\n",
       "           4.64320456,   5.95562703],\n",
       "        [  2.68172565,   4.69111392,   6.00752987, ...,   2.33313895,\n",
       "           4.88327405,   5.89319352],\n",
       "        ...,\n",
       "        [  2.88615944,   4.58169968,   7.03203796, ...,   2.85870114,\n",
       "           6.43607664,   8.20746451],\n",
       "        [  3.82111284,   6.147391  ,   7.96404292, ...,   5.18962595,\n",
       "           9.02073139,  10.52202717],\n",
       "        [  3.0952573 ,   5.09163289,   7.2511643 , ...,   3.68223658,\n",
       "           6.45834201,   7.93231167]],\n",
       "\n",
       "       [[ -3.55292048,  -2.57734166,   0.46988764, ...,  -2.667212  ,\n",
       "           0.33059476,   1.62702019],\n",
       "        [ -4.27792232,  -2.89679894,   0.10012004, ...,  -3.9008292 ,\n",
       "          -0.70520562,   1.30214534],\n",
       "        [ -3.6991242 ,  -2.66982895,   0.78891991, ...,  -3.18706886,\n",
       "          -0.79972626,   0.95797148],\n",
       "        ...,\n",
       "        [ -2.85973553,  -1.43851253,   2.06235579, ...,  -2.32214501,\n",
       "           0.87769816,   2.03116027],\n",
       "        [ -2.72099274,  -1.2487509 ,   1.94669351, ...,  -1.31382187,\n",
       "           1.2177659 ,   2.28010098],\n",
       "        [ -3.31538344,  -2.6987252 ,   0.48891109, ...,  -3.96147854,\n",
       "          -0.48484123,   0.78764583]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-16.68180603, -16.30406716, -16.25615912, ..., -26.3324061 ,\n",
       "         -27.27316542, -27.66824345],\n",
       "        [-16.57646355, -16.34540797, -16.24648419, ..., -25.44334691,\n",
       "         -26.66070009, -27.2563459 ],\n",
       "        [-15.68368673, -15.41502132, -15.41156617, ..., -25.29771944,\n",
       "         -26.64718776, -27.49169997],\n",
       "        ...,\n",
       "        [-15.05927115, -14.86850304, -14.81237362, ..., -24.26314293,\n",
       "         -25.3332853 , -25.65772692],\n",
       "        [-11.54493886, -11.4711934 , -11.44979426, ..., -23.0556308 ,\n",
       "         -24.04001686, -24.4873564 ],\n",
       "        [-13.33606589, -13.07815317, -13.07279189, ..., -25.42955245,\n",
       "         -26.46674102, -26.43785721]],\n",
       "\n",
       "       [[-18.87515065, -18.81329201, -18.75438415, ..., -35.22040977,\n",
       "         -36.57833534, -36.75390974],\n",
       "        [-18.78041445, -18.70814949, -18.74196027, ..., -32.946928  ,\n",
       "         -33.9590805 , -34.48899365],\n",
       "        [-18.90119789, -18.85843404, -18.7882333 , ..., -33.52659683,\n",
       "         -34.57276626, -34.91448424],\n",
       "        ...,\n",
       "        [-17.90763786, -17.92692819, -17.87687178, ..., -29.74137111,\n",
       "         -30.18366014, -30.51742071],\n",
       "        [-17.89830239, -17.89854726, -17.86322299, ..., -27.71205959,\n",
       "         -28.08721319, -28.30945205],\n",
       "        [-17.52012398, -17.48752099, -17.45015736, ..., -31.87651154,\n",
       "         -32.70700476, -32.92166801]],\n",
       "\n",
       "       [[-22.37794214, -22.36937814, -22.37335667, ..., -42.12724652,\n",
       "         -42.81805673, -43.15437603],\n",
       "        [-22.27326436, -22.26061015, -22.27411712, ..., -37.86522687,\n",
       "         -38.36166251, -38.63992133],\n",
       "        [-22.21693547, -22.21625   , -22.21994489, ..., -39.65570409,\n",
       "         -40.20433785, -40.42028135],\n",
       "        ...,\n",
       "        [-22.02281587, -22.02232942, -22.03167753, ..., -35.80181734,\n",
       "         -36.15280428, -36.38344525],\n",
       "        [-21.85360541, -21.86075155, -21.8824183 , ..., -33.5741312 ,\n",
       "         -33.87022888, -34.15476573],\n",
       "        [-22.12309373, -22.12539926, -22.1368384 , ..., -36.1596641 ,\n",
       "         -36.57587715, -36.88193088]]])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_generator(processed_info_dir, first_day_data, selected_picks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "0d3b04ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_mean_all=[]\n",
    "df_peak_all=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68263df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "        data_epo=combined_data_trials\n",
    "        print(data_epo.shape)\n",
    "        data_epo_zscore=np.copy(data_epo)\n",
    "        for i in range(data_epo.shape[0]): # for each epoch\n",
    "            for j in range(data_epo.shape[1]): # for each channel\n",
    "                # compute mean and std of baseline\n",
    "                base_mean = np.mean(data_epo[i,j,:len_baseline_samples])\n",
    "                base_std = np.std(data_epo[i,j,:len_baseline_samples])\n",
    "\n",
    "                # compute z-scored data from baseline stats\n",
    "                data_epo_zscore[i,j,:] = (data_epo[i,j,:]-base_mean)/base_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9295d303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1711115917.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(sub_fname)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "df_mean_all=[]\n",
    "df_peak_all=[]\n",
    "\n",
    "#loop through to analyze each subject - Redundant for SEEG Data\n",
    "\"\"\"for sub_folder in os.listdir(parent_data_dir):\n",
    "    sub_num='' # reset\n",
    "    # ignores hidden files\n",
    "    if sub_folder.startswith('.') or sub_folder not in chosen_list:\n",
    "        continue\n",
    "    elif sub_folder in chosen_list:\n",
    "        if not os.path.exists(os.path.join(save_dir,sub_folder)):\n",
    "            os.mkdir(os.path.join(save_dir,sub_folder))        \n",
    "        sub_num=sub_folder\n",
    "\n",
    "\n",
    "    data_dir = os.path.join(parent_data_dir,sub_num)\n",
    "\n",
    "    print(f\"{sub_num}\\nReading data...\\n\")\n",
    "\n",
    "    conditions=[]\"\"\"\n",
    "\n",
    "display.clear_output(wait=True) # Not sure\n",
    "\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ LOAD SUBJECT DATA @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "\n",
    "    \n",
    "tmin, tmax = 0, 300  # set the timeframe of data we will use (start, end)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "for file in os.listdir(processed_info_dir):\n",
    "    if file.endswith('.fif') and not file.endswith('512-raw.fif'):\n",
    "        raw_fname = os.path.join(processed_info_dir, str(file))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ LOAD PROCESSED INFO @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "    \n",
    "\n",
    "    # # Load in pain rating for each stimuli - pain ratings from Intracranial subjects are WIP\n",
    "    # pain_ratings = scio.loadmat(os.path.join(processed_info_dir,sub_num+'_pain_ratings.mat'))\n",
    "    # pain_ratings = pain_ratings['pain_ratings'].tolist()[0]\n",
    "    # print(f\"*pain_ratings.size = {len(pain_ratings)}*\\n\")\n",
    "\n",
    "    # # Load in drop log for bad trials\n",
    "    # drop_log = scio.loadmat(os.path.join(processed_info_dir,sub_num+'_drop_log.mat'))\n",
    "    # drop_log = drop_log['drop_log'] # leave as array\n",
    "    # print(f\"*drop_log.size = {drop_log.shape[0]}*\\n\")\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ COMPUTE PSD FROM RAW FIF @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "\n",
    "    # TODO: implement following tutorial for psd from raw.fif\n",
    "    # https://mne.tools/0.21/auto_examples/time_frequency/plot_compute_raw_data_spectrum.html\n",
    "    # done\n",
    "\n",
    "    \n",
    "    # Extract PSD into each frequency band of interest\n",
    "    broad1 = epochs.compute_psd(fmin=1., fmax=55., n_jobs=-1) \n",
    "    broad2 = epochs.compute_psd(fmin=65., fmax=MUA_max., n_jobs=-1) # avoid the notch filter\n",
    "    delta = epochs.compute_psd(fmin=1., fmax=4., n_jobs=-1)\n",
    "    theta = epochs.compute_psd(fmin=4., fmax=8., n_jobs=-1)\n",
    "    alpha = epochs.compute_psd(fmin=8., fmax=13., n_jobs=-1)\n",
    "    beta = epochs.compute_psd(fmin=13., fmax=30., n_jobs=-1)\n",
    "    low_gamma = epochs.compute_psd(fmin=30., fmax=55., n_jobs=-1)\n",
    "    high_gamma = epochs.compute_psd(fmin=65., fmax=100., n_jobs=-1)\n",
    "    MUA = fmin = 100, fmax = MUA_max\n",
    "    \n",
    "    # Get data (shape = epochs x channels x powers in each band (1 hz width))\n",
    "    psds_broad1 = broad1.get_data()\n",
    "    psds_broad2 = broad2.get_data()\n",
    "    psds_delta = delta.get_data()\n",
    "    psds_theta = theta.get_data()\n",
    "    psds_alpha = alpha.get_data()\n",
    "    psds_beta = beta.get_data() \n",
    "    psds_low_gamma = low_gamma.get_data()\n",
    "    psds_high_gamma = high_gamma.get_data()\n",
    "\n",
    "    # Get average power across bands (shape = epochs x channels)\n",
    "    avg_psd_broad1 = np.mean(psds_broad1, axis=2)\n",
    "    avg_psd_broad2 = np.mean(psds_broad1, axis=2)\n",
    "    avg_psd_delta = np.mean(psds_delta, axis=2)\n",
    "    avg_psd_theta = np.mean(psds_theta, axis=2)\n",
    "    avg_psd_alpha = np.mean(psds_alpha, axis=2)\n",
    "    avg_psd_beta = np.mean(psds_beta, axis=2)\n",
    "    avg_psd_low_gamma = np.mean(psds_low_gamma, axis=2)\n",
    "    avg_psd_high_gamma = np.mean(psds_high_gamma, axis=2)\n",
    "\n",
    "    # Combine broad spectrum powers and take the trace\n",
    "    avg_psd_broad = np.mean( np.array([avg_psd_broad1, avg_psd_broad2 ]), axis=0 )\n",
    "    trace_avg_psd_broad = np.trace(avg_psd_broad)\n",
    "    print(\"Trace of the average broad spectrum power: \",trace_avg_psd_broad)\n",
    "\n",
    "    # Get band PSD's normalized by the trace of the broad spectrum and convert to dB\n",
    "    avg_psd_delta = 10 * np.log10(avg_psd_delta / trace_avg_psd_broad)\n",
    "    avg_psd_theta = 10 * np.log10(avg_psd_theta / trace_avg_psd_broad)\n",
    "    avg_psd_alpha = 10 * np.log10(avg_psd_alpha / trace_avg_psd_broad)\n",
    "    avg_psd_beta = 10 * np.log10(avg_psd_beta / trace_avg_psd_broad)\n",
    "    avg_psd_low_gamma = 10 * np.log10(avg_psd_low_gamma / trace_avg_psd_broad)\n",
    "    avg_psd_high_gamma = 10 * np.log10(avg_psd_high_gamma / trace_avg_psd_broad)\n",
    "\n",
    "    avg_psd_arr = np.array((avg_psd_delta, avg_psd_theta, \n",
    "                            avg_psd_alpha, avg_psd_beta,\n",
    "                            avg_psd_low_gamma, avg_psd_high_gamma))\n",
    "    \n",
    "    # # TODO: you may or may not need to use the following code block but im leaving it here just in case \n",
    "    # data_chs_array = np.zeros((len(roi_names),len(epo_times),Fs))\n",
    "    # data_trials_tmp=np.zeros((len(epo_times),Fs))\n",
    "    # for j,raw in enumerate(raw_objects_lst): # for each roi\n",
    "    #     extracted_data_tmp=raw.data\n",
    "    #     for i in range(len(epo_times)): # for each trial\n",
    "    #         #all vertices\n",
    "    #         all_vertices_tmp=extracted_data_tmp[:,int(epo_times[i]+tmin*Fs):int(epo_times[i]+tmax*Fs)]\n",
    "    #         #averaged vertices\n",
    "    #         vertices_averaged_tmp=np.mean(all_vertices_tmp,axis=0)\n",
    "    #         #store trials x time data in numpy array\n",
    "    #         data_trials_tmp[i,:] = vertices_averaged_tmp\n",
    "    #     data_chs_array[j,...] = data_trials_tmp\n",
    "    # # data in Array format \n",
    "    # data_chs_corrected_array=np.transpose(data_chs_array,(1,0,2))\n",
    "\n",
    "    # Z-score data by for loop\n",
    "    # TODO: z-score the data on a segment of the last-day data, and exclude that segment from the last-day data to be analyzed\n",
    "    data_epo=combined_data_trials\n",
    "    print(data_epo.shape)\n",
    "    data_epo_zscore=np.copy(data_epo)\n",
    "    for i in range(data_epo.shape[0]): # for each epoch\n",
    "        for j in range(data_epo.shape[1]): # for each channel\n",
    "            # compute mean and std of baseline\n",
    "            base_mean = np.mean(data_epo[i,j,:len_baseline_samples])\n",
    "            base_std = np.std(data_epo[i,j,:len_baseline_samples])\n",
    "        \n",
    "            # compute z-scored data from baseline stats\n",
    "            data_epo_zscore[i,j,:] = (data_epo[i,j,:]-base_mean)/base_std\n",
    "\n",
    "    # compare data before and after z-score\n",
    "    print(data_epo[0,0,len_baseline_samples:len_baseline_samples+10],'\\n')\n",
    "    print(data_epo_zscore[0,0,len_baseline_samples:len_baseline_samples+10])\n",
    "\n",
    "    # TODO: epochs object will just have z-scored day 1 data and last day data\n",
    "    # Create info object required for epochs object\n",
    "    info = mne.create_info(roi_names, sfreq=Fs,ch_types='eeg')\n",
    "    data_epo = data_chs_corrected_array\n",
    "    # Create events array for Epochs object\n",
    "    # TODO: 0*i means the start time from each piece of data will just be 0\n",
    "    # because we dont have any epochs in the data, we are treating the entire thing\n",
    "    # as one epoch.\n",
    "    # NOTE: the i in the last term of the list should just populate 0 and then 1,\n",
    "    # indicating day 1 as label of 0 and last day as label of 1.\n",
    "    \n",
    "    events=np.array([[0*i,0,i] for i in range(len(conditions))])\n",
    "\n",
    "    zepochs = mne.EpochsArray(data=data_epo_zscore,\n",
    "                              info=info,\n",
    "                              tmin=tmin,\n",
    "                              events=events,\n",
    "                              event_id=event_ids_dict,\n",
    "                              baseline=(None,bmax),\n",
    "                              )\n",
    "    print(zepochs)\n",
    "    epochs=zepochs \n",
    "    del zepochs\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ CONSTRUCT TFR OBJECT FOR DF @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "    \n",
    "    # TODO: do not implement ERDS, leaving this code here to figure out how to use it to set up the dataframe\n",
    "    tfr = tfr_multitaper(\n",
    "        epochs,\n",
    "        freqs=freqs,\n",
    "        n_cycles=freqs,\n",
    "        use_fft=True,\n",
    "        return_itc=False,\n",
    "        average=False,\n",
    "        decim=2,\n",
    "    )\n",
    "    tfr.crop(tmin, tmax).apply_baseline(baseline, mode=\"percent\")\n",
    "\n",
    "    for event in event_ids:\n",
    "        # select desired epochs for visualization\n",
    "        tfr_ev = tfr[event]\n",
    "        fig, axes = plt.subplots(\n",
    "            1, 4, figsize=(12, 4), gridspec_kw={\"width_ratios\": [10, 10, 10, 1]}\n",
    "        )\n",
    "        for ch, ax in enumerate(axes[:-1]):  # for each channel\n",
    "            # positive clusters\n",
    "            _, c1, p1, _ = pcluster_test(tfr_ev.data[:, ch], tail=1, **kwargs)\n",
    "            # negative clusters\n",
    "            _, c2, p2, _ = pcluster_test(tfr_ev.data[:, ch], tail=-1, **kwargs)\n",
    "\n",
    "            # note that we keep clusters with p <= 0.05 from the combined clusters\n",
    "            # of two independent tests; in this example, we do not correct for\n",
    "            # these two comparisons\n",
    "            c = np.stack(c1 + c2, axis=2)  # combined clusters\n",
    "            p = np.concatenate((p1, p2))  # combined p-values\n",
    "            mask = c[..., p <= 0.05].any(axis=-1)\n",
    "\n",
    "            # plot TFR (ERDS map with masking)\n",
    "            tfr_ev.average().plot(\n",
    "                [ch],\n",
    "                cmap=\"RdBu\",\n",
    "                cnorm=cnorm,\n",
    "                axes=ax,\n",
    "                colorbar=False,\n",
    "                show=False,\n",
    "                mask=mask,\n",
    "                mask_style=\"mask\",\n",
    "            )\n",
    "\n",
    "            ax.set_title(epochs.ch_names[ch], fontsize=10)\n",
    "            ax.axvline(0, linewidth=1, color=\"black\", linestyle=\":\")  # event\n",
    "            if ch != 0:\n",
    "                ax.set_ylabel(\"\")\n",
    "                ax.set_yticklabels(\"\")\n",
    "        fig.colorbar(axes[0].images[-1], cax=axes[-1]).ax.set_yscale(\"linear\")\n",
    "        fig.suptitle(f\"ERDS ({event})\")\n",
    "        plt.show()\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ SET UP PANDAS DATAFRAMES @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "\n",
    "    #Export tfr to pandas df in seconds\n",
    "    df = tfr.to_data_frame(time_format=None, long_format=False)\n",
    "    df.head()\n",
    "\n",
    "    #Plot with confidence bands \n",
    "    df = tfr.to_data_frame(time_format=None, long_format=True)\n",
    "\n",
    "    # Map to frequency bands:\n",
    "    freq_bounds = {'_': 0,\n",
    "                   'delta': 4,\n",
    "                   'theta': 7,\n",
    "                   'alpha': 13,\n",
    "                   'beta': 35,\n",
    "                   'low-gamma': 55,\n",
    "                   'notch': 65,\n",
    "                   'high-gamma': 100,\n",
    "                   'MUA': 500\n",
    "                  }\n",
    "\n",
    "    df['band'] = pd.cut(df['freq'], list(freq_bounds.values()),\n",
    "                        labels=list(freq_bounds)[1:])\n",
    "\n",
    "    # Filter to retain only relevant frequency bands:\n",
    "    freq_bands_of_interest = [\n",
    "                              # 'delta', \n",
    "                              'theta', \n",
    "                              'alpha', \n",
    "                              'beta', \n",
    "                              'low-gamma', \n",
    "                              'high-gamma',\n",
    "                              'MUA',\n",
    "        ]\n",
    "    df = df[df.band.isin(freq_bands_of_interest)]\n",
    "    df['band'] = df['band'].cat.remove_unused_categories()\n",
    "\n",
    "    # Order channels for plotting:\n",
    "    df['channel'] = df['channel'].cat.reorder_categories(tuple(roi_names),\n",
    "                                                         ordered=True)\n",
    "\n",
    "    g = sns.FacetGrid(df, row='band', col='channel', margin_titles=True)\n",
    "    g.map(sns.lineplot, 'time', 'value', n_boot=10)\n",
    "    axline_kw = dict(color='black', linestyle='dashed', linewidth=0.5, alpha=0.5)\n",
    "    g.map(plt.axhline, y=0, **axline_kw)\n",
    "    g.map(plt.axvline, x=0, **axline_kw)\n",
    "    g.set(xlim=(tmin+0.5,tmax-0.5))\n",
    "    # g.set(ylim=(None, 3))\n",
    "    g.set_axis_labels(\"Time (s)\", \"Z-scored Band Power (uV^2)\")\n",
    "    g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
    "    g.add_legend(ncol=2, loc='lower center')\n",
    "    g.fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.08)\n",
    "    plt.show();\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    # Plot the ERDS based on average, peak amplitude, and area-under-the-curve (AUC), one by one.\n",
    "    df_mean = (df.query(f\"time > {tmin}\")\n",
    "                 .groupby(['epoch','band','channel'])[['value']]\n",
    "                 .mean()\n",
    "                 .dropna(subset=['value']) # needed to remove taking average over non-numeric values\n",
    "                 .reset_index())\n",
    "\n",
    "    df_mean_all.append(df_mean) #when it loops through each subject, will save the mean in the array\n",
    "\n",
    "    df_peak = (df.query(f\"time > {tmin} & time < {tmax}\")\n",
    "                 .groupby(['epoch','band','channel'])[['value']]\n",
    "                 .max() # peak amplitude\n",
    "                 .dropna(subset=['value']) # needed to remove taking average over non-numeric values\n",
    "                 .reset_index())\n",
    "    df_peak_all.append(df_peak) #when it loops through each subject, will save the peak\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ EVALUATE AND SAVE STATS OUTPUTS @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "    stats_plot(df_mean, save_dir, 'mean')\n",
    "    stats_plot(df_peak, save_dir, 'peak')\n",
    "    \n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794755a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae315a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42048b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegenv",
   "language": "python",
   "name": "eegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
