{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14889b5a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_22719/3243943048.py:21: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "import scipy.io as scio\n",
    "import os\n",
    "import mne\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "import matplotlib.image as mpimg\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from mne.time_frequency import tfr_multitaper\n",
    "from mne.stats import permutation_cluster_1samp_test as pcluster_test\n",
    "import scipy.stats as stats\n",
    "from statannotations.Annotator import Annotator\n",
    "%matplotlib inline\n",
    "# sns.set(style=\"whitegrid\")\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b18f6c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "501d8e86",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def delete_multiple_elements(list_object, indices):\n",
    "    indices = sorted(indices, reverse=True)\n",
    "    for idx in indices:\n",
    "        if idx < len(list_object):\n",
    "            list_object.pop(idx) # define functions for extracting relevant epochs\n",
    "            \n",
    "def stats_plot(df, save_dir, feature, file_name):\n",
    "    '''\n",
    "    df = dataframe (mean, peak)\n",
    "    save_dir = directory to save stats PNG and TXT files\n",
    "    feature = 'mean','peak'\n",
    "    '''    \n",
    "\n",
    "    pairs=[((f,conditions[0]),(f,conditions[1])) for f in freq_bands_of_interest]\n",
    "\n",
    "    for roi in roi_names:        \n",
    "        df_ch_tmp=df.query(f\"channel=='{roi}'\")\n",
    "       \n",
    "\n",
    "        #stats plot\n",
    "        plt.figure(figsize=(8.6,6.5));\n",
    "        ax_tmp = sns.boxplot(data=df_ch_tmp,x='band', y='value', hue='condition',\n",
    "                palette='pastel', order=freq_bands_of_interest,\n",
    "                hue_order=conditions,\n",
    "                linewidth=0.5)\n",
    "\n",
    "        ax_tmp.set_title(f\"{roi} | Resting state Power | Feature: '{feature}' \",pad=30)\n",
    "\n",
    "        #stats output\n",
    "        x='band'\n",
    "        y='value'\n",
    "        hue='condition'\n",
    "        order=freq_bands_of_interest\n",
    "        hue_order = conditions\n",
    "\n",
    "        test = 'Mann-Whitney'\n",
    "        print(f\"******************************************************{roi} {test}******************************************************\")\n",
    "        annot = Annotator(ax_tmp, pairs, data=df_ch_tmp, x=x, y=y, hue=hue, order=order,hue_order=hue_order)\n",
    "        annot.configure(test=test, text_format='star', loc='outside',comparisons_correction=None)\n",
    "        annot.apply_test()\n",
    "        annot.annotate()\n",
    "\n",
    "        # save plot and tfr data\n",
    "        # TODO: follow same naming convention as in preprocessing code\n",
    "        # Add region to end, removed raw.fif\n",
    "        save_fname = f\"{file_name[:-4]}_{roi}\"\n",
    "        \n",
    "        # plot as png\n",
    "        plt.savefig(os.path.join(save_dir,sub_folder,save_fname+\".png\"))\n",
    "        plt.figure()\n",
    "        \n",
    "        # tfr data as csv\n",
    "        df_ch_tmp.to_csv(os.path.join(save_dir,sub_folder,save_fname+\".csv\"))\n",
    "        display.clear_output(wait=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4baa6b4-fbe7-427a-83de-7f65b70f3ce0",
   "metadata": {},
   "source": [
    "PSD Generation and Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "add7670f-a0da-4cd1-acbb-4da989b6ec21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# File pair generator\n",
    "\n",
    "def file_pair_generator(file_dir, sub_id, sub_state):\n",
    "    same_state_double_pairs = []\n",
    "    day_1_data = []\n",
    "    day_last_data = []\n",
    "    for file in os.listdir(file_dir):\n",
    "        if str(sub_id) in file and str(sub_state) in file and not '512' in file:\n",
    "            same_state_double_pairs.append(file)\n",
    "    for i in range(0, len(same_state_double_pairs)):\n",
    "        if 'Day 1' in same_state_double_pairs[i]:\n",
    "            day_1_data.append(same_state_double_pairs[i])\n",
    "        else:\n",
    "            day_last_data.append(same_state_double_pairs[i])\n",
    "    return day_1_data, day_last_data\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5f6060b-2c9f-4fb3-ba2f-b6815a066be9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Functions are divided into surface and depth generators, surface generator allows users to select chosen nodes given image data\n",
    "# depth generator used for when images can offer no better info on channel location\n",
    "\n",
    "def ROI_Surface_Generator(ROI_Data_Path, ROI_sub_id, surface_side): # surface side must be 'lh' (lefthand) or 'rh' (righthand)\n",
    "    for file in os.listdir(ROI_Data_Path):\n",
    "        if str(ROI_sub_id) in file and not str('depth') in file and str(surface_side) in file:\n",
    "            processed_roi_csv = pd.read_csv(os.path.join(ROI_Data_Path, file))\n",
    "\n",
    "    ROI_channels = processed_roi_csv['ID'].to_list()\n",
    "    Chosen_ROI_Channels = [] # reset\n",
    "    channel_ids = 0\n",
    "    \n",
    "    while channel_ids != len(ROI_channels):\n",
    "        verification_status = input(f\"Proposed Channel in Region of Interest: {ROI_channels[channel_ids]} - Verify from Image, Keep y/n ?\")\n",
    "        if verification_status == 'y':\n",
    "            Chosen_ROI_Channels.append(ROI_channels[channel_ids])\n",
    "            channel_ids = channel_ids + 1\n",
    "            display.clear_output(wait=True)\n",
    "        elif verification_status == 'n':\n",
    "            channel_ids = channel_ids + 1\n",
    "            display.clear_output(wait=True)\n",
    "        else:\n",
    "            print(\"Invalid Reponse, please enter 'y' or 'n'\")\n",
    "            channel_ids = channel_ids + 0\n",
    "            display.clear_output(wait=True)\n",
    "            \n",
    "    return(Chosen_ROI_Channels) # returns chosen channels as list of channel IDs\n",
    "\n",
    "\n",
    "\n",
    "def ROI_Depth_Generator(ROI_Data_Path, ROI_sub_id): # use for files containing 'depth'\n",
    "    for file in os.listdir(ROI_Data_Path):\n",
    "        if str(ROI_sub_id) in file and str('depth') in file:\n",
    "            processed_roi_csv = pd.read_csv(os.path.join(ROI_Data_Path, file))\n",
    "    \n",
    "    ROI_channels = processed_roi_csv['ID'].to_list()\n",
    "    return(ROI_channels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78762c37-4bdf-443c-bf89-0b0b3dbde607",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Automate the generation of broad or MUA (>100 Hz) psds and combine them\n",
    "\n",
    "def notch_filter_analysis(broad_or_MUA, generated_epochs, avg_psd_broad_lower, picks): # enter 'broad' or 'MUA' for broad_or_MUA argument\n",
    "    if broad_or_MUA == 'broad':\n",
    "        broad_band_list = [] # reset\n",
    "        for i in range(0, (int(MUA_max/60)-1)):\n",
    "            if i == 0:\n",
    "                broad_band = generated_epochs.compute_psd(fmin=65., fmax=117., picks=picks, n_jobs=n_cores)\n",
    "                avg_psd_broad_band = np.mean(broad_band, axis=2)\n",
    "                broad_band_list.append(avg_psd_broad_band)\n",
    "            elif i == (int(MUA_max/60)-1):\n",
    "                broad_band = generated_epochs.compute_psd(fmin=63+i*60., fmax = MUA_max, picks = picks, n_jobs=n_cores)\n",
    "                avg_psd_broad_band = np.mean(broad_band, axis=2)\n",
    "                broad_band_list.append(avg_psd_broad_band)\n",
    "            else:\n",
    "                broad_band = generated_epochs.compute_psd(fmin=63+i*60., fmax=57+(i+1)*60., picks=picks, n_jobs=n_cores)\n",
    "                avg_psd_broad_band = np.mean(broad_band, axis=2)\n",
    "                broad_band_list.append(avg_psd_broad_band)\n",
    "        broad_band_list.append(avg_psd_broad_lower)\n",
    "        avg_psd_broad = np.mean(np.array(broad_band_list), axis=0)\n",
    "        return(avg_psd_broad)\n",
    "    elif broad_or_MUA == 'MUA':\n",
    "        MUA_band_list = [] # reset\n",
    "        for i in range(0, (int(MUA_max/60)-1)):\n",
    "            if i == 0:\n",
    "                MUA_band = generated_epochs.compute_psd(fmin=100., fmax = 117., picks=picks, n_jobs=n_cores)\n",
    "                avg_psd_MUA_band = np.mean(MUA_band, axis=2)\n",
    "                MUA_band_list.append(avg_psd_MUA_band)\n",
    "            elif i == int(MUA_max/60-1):\n",
    "                MUA_band = generated_epochs.compute_psd(fmin=63+i*60, fmax = MUA_max, picks=picks, n_jobs=n_cores)\n",
    "                avg_psd_MUA_band = np.mean(MUA_band, axis=2)\n",
    "                MUA_band_list.append(avg_psd_MUA_band)\n",
    "            else:\n",
    "                MUA_band = generated_epochs.compute_psd(fmin = 63+i*60., fmax = 117+i*60., picks=picks, n_jobs=n_cores)\n",
    "                avg_psd_MUA_band = np.mean(MUA_band, axis=2)\n",
    "                MUA_band_list.append(avg_psd_MUA_band)\n",
    "        avg_psd_MUA = np.mean(np.array(MUA_band_list), axis=0)\n",
    "        return(avg_psd_MUA)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "926607c0-6a11-40df-9166-f10b0248ce01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def avg_psd_arr_generator(fif_file_path, file_pair, picks):\n",
    "    avg_psd_arr_lst = [] # reset\n",
    "    for i in range(0, len(file_pair)):\n",
    "        raw = mne.io.read_raw_fif(os.path.join(fif_file_path, str(file_pair[i]))).crop(tmin, tmax).load_data()\n",
    "        print(f\"processing data for {file_pair[i]}\")\n",
    "        raw = raw.notch_filter(np.arange(120, 481, 60), n_jobs=-1, fir_design='firwin', notch_widths = 6) # inital notch filter for all harmonics\n",
    "        epochs = mne.make_fixed_length_epochs(raw, duration=30, preload=False)\n",
    "        n_fft = 2048  # the FFT size (n_fft). Ideally a power of 2\n",
    "\n",
    "    # Calculate the psds from raw data based on frequency range\n",
    "        broad1 = epochs.compute_psd(fmin=1., fmax=55., picks=picks, n_jobs=n_cores)\n",
    "        delta = epochs.compute_psd(fmin=1., fmax=4., picks=picks, n_jobs=n_cores)\n",
    "        theta = epochs.compute_psd(fmin=4., fmax=8., picks=picks, n_jobs=n_cores)\n",
    "        alpha = epochs.compute_psd(fmin=8., fmax=13., picks=picks, n_jobs=n_cores)\n",
    "        beta = epochs.compute_psd(fmin=13., fmax=30., picks=picks, n_jobs=n_cores)\n",
    "        low_gamma = epochs.compute_psd(fmin=30., fmax=55., picks=picks, n_jobs=n_cores)\n",
    "        high_gamma = epochs.compute_psd(fmin=65., fmax=100., picks=picks, n_jobs=n_cores)\n",
    "    \n",
    "    \n",
    "    # Get average power across bands (shape = epochs x channels)\n",
    "        avg_psd_broad1 = np.mean(broad1, axis=2)\n",
    "        avg_psd_delta = np.mean(delta, axis=2)\n",
    "        avg_psd_theta = np.mean(theta, axis=2)\n",
    "        avg_psd_alpha = np.mean(alpha, axis=2)\n",
    "        avg_psd_beta = np.mean(beta, axis=2)\n",
    "        avg_psd_low_gamma = np.mean(low_gamma, axis=2)\n",
    "        avg_psd_high_gamma = np.mean(high_gamma, axis=2)\n",
    "        \n",
    "    # Calculate the average MUA and larger broad frequencies using the notch filter analysis function\n",
    "        avg_psd_broad = notch_filter_analysis('broad', epochs, avg_psd_broad1, picks)\n",
    "        avg_psd_MUA = notch_filter_analysis('MUA', epochs, avg_psd_broad1, picks) # Final argument is neccecary for funtion, but plays no part in MUA calculations\n",
    "        display.clear_output()\n",
    "        print(f\"applying notch filter across all epochs for broad2 analysis and MUA frequencies\")\n",
    "\n",
    "\n",
    "    # Combine broad spectrum powers and take the trace\n",
    "        trace_avg_psd_broad = np.trace(avg_psd_broad)\n",
    "        display.clear_output()\n",
    "        print(f\"Trace of the average broad spectrum power: {trace_avg_psd_broad}\")    \n",
    "\n",
    "    # Get band PSD's normalized by the trace of the broad spectrum and convert to dB\n",
    "        avg_psd_delta = 10 * np.log10(avg_psd_delta / trace_avg_psd_broad)\n",
    "        avg_psd_theta = 10 * np.log10(avg_psd_theta / trace_avg_psd_broad)\n",
    "        avg_psd_alpha = 10 * np.log10(avg_psd_alpha / trace_avg_psd_broad)\n",
    "        avg_psd_beta = 10 * np.log10(avg_psd_beta / trace_avg_psd_broad)\n",
    "        avg_psd_low_gamma = 10 * np.log10(avg_psd_low_gamma / trace_avg_psd_broad)\n",
    "        avg_psd_high_gamma = 10 * np.log10(avg_psd_high_gamma / trace_avg_psd_broad)\n",
    "        avg_psd_MUA = 10 * np.log10(avg_psd_MUA / trace_avg_psd_broad)\n",
    "\n",
    "        avg_psd_arr = np.array((avg_psd_delta, avg_psd_theta, \n",
    "                            avg_psd_alpha, avg_psd_beta,\n",
    "                            avg_psd_low_gamma, avg_psd_high_gamma, avg_psd_MUA)) \n",
    "\n",
    "        avg_psd_arr_lst.append(avg_psd_arr)\n",
    "\n",
    "    ID_sub_state_psd_arr = np.concatenate(avg_psd_arr_lst, axis = 0)\n",
    "    return(ID_sub_state_psd_arr)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5b82f62-d040-4bb1-b3f7-b727c099cec1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Z-Score Calculation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a617b8df-620b-4cd2-a177-bd3aa82081dd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of time frames is 30.0 seconds\n"
     ]
    }
   ],
   "source": [
    "# Z-SCORE CALCULATION SETTINGS\n",
    "\n",
    "# 1. Choose Asleep1, Asleep2, Awake1, or Awake2 - Arbitrary from current understanding, doesn't really matter for calculations\n",
    "zsub_state = 'Asleep1'\n",
    "\n",
    "# 2. Choose time frame of baseline z-score - VALUES MUST BE FLOAT (decimal at the end)\n",
    "# IMPORTANT: note that all future time frames must match the baseline in length\n",
    "ztmin, ztmax = 270., 300.\n",
    "print(f\"Length of time frames is {ztmax - ztmin} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7825f111-b68b-4723-bc4a-e483164b27d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Take the data file given the subject ID and sub_state, to generate baseline time frame\n",
    "# Uses the time frame selected in Z-Score settings and must be given a specific channel to operate\n",
    "# Use the ROI generators to find channels of interest\n",
    "\n",
    "def z_score_baseline(data_dir, ztmin, ztmax, sub_id, zsub_state):\n",
    "    for files in os.listdir(data_dir):\n",
    "        if str(sub_id) in files and str(zsub_state) in files and not 'Day 1' in files:\n",
    "            file_name = files\n",
    "    raw = mne.mne.io.read_raw_fif(os.path.join(data_dir, file_name))\n",
    "    raw_df = raw_df[raw_df['time'] > ztmin and raw_df['time'] < ztmax] \n",
    "    base_mean = np.mean(raw_df)\n",
    "    base_std = np.std(raw_df)\n",
    "    return base_mean, base_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "524c984b-30d0-472a-af27-4cbdc2489ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def z_score_calculator(data_dir, zsub_state, ztmin, ztmax, sub_id, picks):\n",
    "    file_list = [] # reset\n",
    "    for file in os.listdir(data_dir):\n",
    "        if sub_id in file and not '512' in file and str(zsub_state)[:-1] in file:\n",
    "            file_list.append(file)\n",
    "            print(f\"file added to analyzing list: {file}\")\n",
    "            \n",
    "    z_score_arr_lst = [] # reset\n",
    "    \n",
    "    for l in range(len(file_list)):\n",
    "        if str(zsub_state) in file_list[l] and not 'Day 1' in file_list[l]:\n",
    "            raw = mne.io.read_raw_fif(os.path.join(data_dir, file_list[l]))\n",
    "            raw_df = raw.to_data_frame(picks = picks)\n",
    "            selected_df = raw_df[raw_df['time'] > ztmin]\n",
    "            selected_df = selected_df[selected_df['time'] < ztmax] \n",
    "            base_mean = np.mean(selected_df)\n",
    "            base_std = np.std(selected_df)\n",
    "            display.clear_output()\n",
    "            print(f\"The baseline mean is: {base_mean} | The baseline std is: {base_std}\")\n",
    "            display.clear_output(wait = True)\n",
    "        \n",
    "    for k in range(len(file_list)):\n",
    "        z_score_list = []\n",
    "        if zsub_state in str(file_list[k]) and not 'Day 1' in str(file_list[k]):\n",
    "            if ztmin > 0 and ztmax < 300:\n",
    "                raw_df = raw_df[raw_df['time'] < ztmin]\n",
    "                raw_df = raw_df[raw_df['time'] > ztmax]\n",
    "            elif ztmax == 300:\n",
    "                raw_df = raw_df[raw_df['time'] < (ztmax - (ztmax-ztmin))]\n",
    "            else:\n",
    "                raw_df = raw_df[raw_df['time'] > (ztmin + (ztmax-ztmin))]\n",
    "            \n",
    "            print(f\"Shape of raw data in file: {file_list[k]}: {raw_df.shape}\")\n",
    "\n",
    "            for i in range(1, (raw_df.shape[1])):\n",
    "                column = raw_df.iloc[:,i].to_list()\n",
    "                z_scores = [] # reset\n",
    "                for j in range(0, len(column)):\n",
    "                    z_score = (column[j] - base_mean[i]) / base_std[i]\n",
    "                    z_scores.append(z_score)\n",
    "            final_day_z_score_arr = np.array(z_score_list)        \n",
    "            print(f\"Process #{k}, for file: {file_list[k]}, shape is {z_score_arr.shape}\")\n",
    "\n",
    "        else:\n",
    "            raw = mne.io.read_raw_fif(os.path.join(data_dir, str(file_list[k])))\n",
    "            raw_df = raw.to_data_frame(picks = picks)\n",
    "            print(f\"Shape of raw data in file: {file_list[k]}: {raw_df.shape}\")\n",
    "            for i in range(1, (raw_df.shape[1])):\n",
    "                column = raw_df.iloc[:,i].to_list()\n",
    "                z_scores = [] # reset\n",
    "                for j in range(0, len(column)):\n",
    "                    z_score = (column[j] - base_mean[i]) / base_std[i] \n",
    "                    z_scores.append(z_score)\n",
    "                z_score_list.append(z_scores)\n",
    "            z_score_arr = np.array(z_score_list)\n",
    "            z_score_arr_lst.append(z_score_arr)\n",
    "            print(f\"Process #{k}, for file: {file_list[k]}, shape is {z_score_arr.shape}\")\n",
    "            \n",
    "    z_score_full_arr = np.stack(z_score_arr_lst, axis=-1) # stacks the individual z score arrays, with 3rd dimension representing each array\n",
    "    return(z_score_full_arr, final_day_z_score_arr) # returns two separate arrays because array for calculating bas\n",
    "    \n",
    "            \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a30439a2-5424-4d9d-b2b3-55ff1686e06e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/NY846_Day 1_Asleep2-raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 615039 =      0.000 ...   300.312 secs\n",
      "Ready.\n",
      "Shape of raw data in file: NY846_Day 1_Asleep2-raw.fif: (615040, 14)\n",
      "Process #0, for file: NY846_Day 1_Asleep2-raw.fif, shape is (13, 615040)\n",
      "Shape of raw data in file: NY846_Day 8_Asleep1-raw.fif: (552960, 14)\n",
      "Process #1, for file: NY846_Day 8_Asleep1-raw.fif, shape is (13, 615040)\n",
      "Opening raw data file ../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/NY846_Day 1_Asleep1-raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 617087 =      0.000 ...   301.312 secs\n",
      "Ready.\n",
      "Shape of raw data in file: NY846_Day 1_Asleep1-raw.fif: (617088, 14)\n",
      "Process #2, for file: NY846_Day 1_Asleep1-raw.fif, shape is (13, 617088)\n",
      "Opening raw data file ../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/NY846_Day 8_Asleep2-raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 617087 =      0.000 ...   301.312 secs\n",
      "Ready.\n",
      "Shape of raw data in file: NY846_Day 8_Asleep2-raw.fif: (617088, 14)\n",
      "Process #3, for file: NY846_Day 8_Asleep2-raw.fif, shape is (13, 617088)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m z_score_test, z_score_train \u001b[38;5;241m=\u001b[39m \u001b[43mz_score_calculator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocessed_info_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mzsub_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mztmin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mztmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msub_ids_lst\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_picks\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 59\u001b[0m, in \u001b[0;36mz_score_calculator\u001b[0;34m(data_dir, zsub_state, ztmin, ztmax, sub_id, picks)\u001b[0m\n\u001b[1;32m     56\u001b[0m         z_score_arr_lst\u001b[38;5;241m.\u001b[39mappend(z_score_arr)\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcess #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, for file: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_list[k]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, shape is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mz_score_arr\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 59\u001b[0m z_score_full_arr \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz_score_arr_lst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# stacks the individual z score arrays, with 3rd dimension representing each array\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m(z_score_full_arr, final_day_z_score_arr)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "File \u001b[0;32m~/anaconda3/envs/eegenv/lib/python3.9/site-packages/numpy/core/shape_base.py:426\u001b[0m, in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    424\u001b[0m shapes \u001b[38;5;241m=\u001b[39m {arr\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m arr \u001b[38;5;129;01min\u001b[39;00m arrays}\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(shapes) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall input arrays must have the same shape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    428\u001b[0m result_ndim \u001b[38;5;241m=\u001b[39m arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    429\u001b[0m axis \u001b[38;5;241m=\u001b[39m normalize_axis_index(axis, result_ndim)\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "z_score_test, z_score_train = z_score_calculator(processed_info_dir, zsub_state, ztmin, ztmax, sub_ids_lst[0], selected_picks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3cdfa47c-f35a-47b0-a691-aa444b75ec38",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting EDF parameters from /media/sb10flpc002/08e63286-43ce-4f61-9491-1ed048c96f20/George Kenefati/Dan Friedman sEEG Data/NY846/Day 1/Asleep2/Asleep2.EDF...\n",
      "EDF file detected\n",
      "Setting channel info structure...\n",
      "Creating raw.info structure...\n",
      "(615040, 14)\n"
     ]
    }
   ],
   "source": [
    "raw = mne.io.read_raw_edf('../../../George Kenefati/Dan Friedman sEEG Data/NY846/Day 1/Asleep2/Asleep2.EDF')\n",
    "raw_df = raw.to_data_frame(picks = selected_picks)\n",
    "print(raw_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13677757-6e32-4dad-be85-5910ed94e476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43910c32-cb2d-4a9b-b87f-e465ab284357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c1239c-529b-45ca-a240-474d3c144f0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b2917f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# GENERAL SETTINGS:\n",
    "\n",
    "# 1. Choose Asleep or Awake data\n",
    "sub_state = 'Asleep'\n",
    "\n",
    "# 2. Set number of CPU cores to use for computing PSD. Default to -1 unless another user is running an intense program\n",
    "n_cores = -1 # 1\n",
    "\n",
    "# 3. Set maximum frequency for MUA \n",
    "MUA_max = 500\n",
    "\n",
    "# 4. Set starting and ending times (in seconds)\n",
    "tmin, tmax = 0, 300\n",
    "\n",
    "# 5. Add in all subject IDs to be analyzed\n",
    "sub_ids_lst = ['846', '853', '865', '870', '871', '872', '878', '884', '888', '893']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "19228b53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data directories\n",
    "processed_info_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/\"\n",
    "parent_data_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/\"\n",
    "save_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/PO1 vs last day stats results/\"\n",
    "processed_roi_dir = f\"../../../George Kenefati/Dan Friedman sEEG Data/Processed Anatomical Regions Data/\"\n",
    "# Globals\n",
    "Fs = 2000 # Hz\n",
    "\n",
    "bmin,bmax = 0,0 # baseline start and end for z-score\n",
    "\n",
    "# TODO: change to appropriate labels for sEEG data. those below are from source space data\n",
    "roi_names = [# Right\n",
    "'rostralanteriorcingulate-rh', # Right Rostral ACC\n",
    "'caudalanteriorcingulate-rh', # Right Caudal ACC\n",
    "'postcentral-rh', # , Right S1\n",
    "'ctx-rh-insula', 'superiorfrontal-rh', # Right Insula, Right DL-PFC\n",
    "'medialorbitofrontal-rh', # Right Medial-PFC\n",
    "# Left\n",
    " 'rostralanteriorcingulate-lh', # Left Rostral ACC\n",
    " 'caudalanteriorcingulate-lh', # Left Caudal ACC\n",
    " 'postcentral-lh', # Left S1,\n",
    " 'ctx-lh-insula', 'superiorfrontal-lh', # Left Insula, Left DL-PFC,\n",
    " 'medialorbitofrontal-lh' # Left Medial-PFC\n",
    "]\n",
    "\n",
    "n_channels = len(roi_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "954b3b06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# List of focus areas:\n",
    "# caudalanteriorcingulate\n",
    "# rostralanteriorcingulate\n",
    "# ctx-lh-insula \n",
    "# ctx-rh-insula \n",
    "# medialorbitofrontal\n",
    "# postcentral (S1)\n",
    "# superiorfrontal (PFC)\n",
    "\n",
    "# Of interest areas:\n",
    "areas_of_interest = ['caudalanteriorcingulate','rostralanteriorcingulate','ctx-lh-insula','ctx-rh-insula','medialorbitofrontal','postcentral','superiorfrontal','insula']\n",
    "\n",
    "# display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "70a35167-c680-46a7-b0de-8e26566ca727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "first_day_data, last_day_data = file_pair_generator(processed_info_dir, sub_ids_lst[0], sub_state)\n",
    "\n",
    "\n",
    "selected_picks = ROI_Depth_Generator(processed_roi_dir, sub_ids_lst[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "099719fc-e63a-451b-813a-c5b8d3bdde88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for i in range(0, len(selected_picks)):\n",
    "    if selected_picks[i][-2:][:-1] == '0':\n",
    "        selected_picks[i] = selected_picks[i][:-2] + selected_picks[i][-1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b94f93ea-be10-4001-bd94-1c974344f8c0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DLI5',\n",
       " 'DLI6',\n",
       " 'DLI7',\n",
       " 'DLI8',\n",
       " 'DLI9',\n",
       " 'DRI1',\n",
       " 'DRI2',\n",
       " 'DRI3',\n",
       " 'DRI4',\n",
       " 'DRI5',\n",
       " 'DRI6',\n",
       " 'DRI7',\n",
       " 'DRI8']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_picks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0d3b04ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Opening raw data file ../../../George Kenefati/Dan Friedman sEEG Data/Processed FIF Data/NY846_Day 1_Asleep1-raw.fif...\n",
      "Isotrak not found\n",
      "    Range : 0 ... 617087 =      0.000 ...   301.312 secs\n",
      "Ready.\n",
      "Reading 0 ... 614400  =      0.000 ...   300.000 secs...\n",
      "Filtering raw data in 1 contiguous segment\n",
      "Setting up band-stop filter\n",
      "\n",
      "FIR filter parameters\n",
      "---------------------\n",
      "Designing a one-pass, zero-phase, non-causal bandstop filter:\n",
      "- Windowed time-domain design (firwin) method\n",
      "- Hamming window with 0.0194 passband ripple and 53 dB stopband attenuation\n",
      "- Lower transition bandwidth: 0.50 Hz\n",
      "- Upper transition bandwidth: 0.50 Hz\n",
      "- Filter length: 13517 samples (6.600 s)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 24 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  72 tasks      | elapsed:    1.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not setting metadata\n",
      "10 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done 191 out of 191 | elapsed:    3.4s finished\n"
     ]
    }
   ],
   "source": [
    "raw = mne.io.read_raw_fif(os.path.join(processed_info_dir, 'NY846_Day 1_Asleep1-raw.fif')).crop(tmin, tmax).load_data()\n",
    "raw = raw.notch_filter(np.arange(120, 481, 60), n_jobs=-1, fir_design='firwin', notch_widths = 6) # inital notch filter for all harmonics\n",
    "epochs = mne.make_fixed_length_epochs(raw, duration=30, preload=False)\n",
    "n_fft = 2048  # the FFT size (n_fft). Ideally a power of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f1c79b29-a567-45bd-af6d-315989fc6ffd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>DLI5</th>\n",
       "      <th>DLI6</th>\n",
       "      <th>DLI7</th>\n",
       "      <th>DLI8</th>\n",
       "      <th>DLI9</th>\n",
       "      <th>DRI1</th>\n",
       "      <th>DRI2</th>\n",
       "      <th>DRI3</th>\n",
       "      <th>DRI4</th>\n",
       "      <th>DRI5</th>\n",
       "      <th>DRI6</th>\n",
       "      <th>DRI7</th>\n",
       "      <th>DRI8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.236668e-13</td>\n",
       "      <td>-1.389134e-13</td>\n",
       "      <td>-4.235165e-14</td>\n",
       "      <td>-1.694066e-13</td>\n",
       "      <td>-1.219727e-13</td>\n",
       "      <td>1.439956e-14</td>\n",
       "      <td>1.118083e-13</td>\n",
       "      <td>2.710505e-14</td>\n",
       "      <td>1.677125e-13</td>\n",
       "      <td>-1.736418e-14</td>\n",
       "      <td>-4.235165e-15</td>\n",
       "      <td>-4.065758e-14</td>\n",
       "      <td>-4.065758e-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000488</td>\n",
       "      <td>-2.430374e+01</td>\n",
       "      <td>-2.510319e+01</td>\n",
       "      <td>-2.527172e+01</td>\n",
       "      <td>-2.646931e+01</td>\n",
       "      <td>-2.484184e+01</td>\n",
       "      <td>-7.125326e+00</td>\n",
       "      <td>-7.385539e+00</td>\n",
       "      <td>-6.276301e+00</td>\n",
       "      <td>-2.353287e+00</td>\n",
       "      <td>-1.578070e+00</td>\n",
       "      <td>-2.794631e+00</td>\n",
       "      <td>-3.933598e+00</td>\n",
       "      <td>-2.874039e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000977</td>\n",
       "      <td>-3.712937e+01</td>\n",
       "      <td>-3.905014e+01</td>\n",
       "      <td>-3.722450e+01</td>\n",
       "      <td>-4.003176e+01</td>\n",
       "      <td>-3.814335e+01</td>\n",
       "      <td>-6.721862e+00</td>\n",
       "      <td>-7.743634e+00</td>\n",
       "      <td>-4.963594e+00</td>\n",
       "      <td>2.268919e+00</td>\n",
       "      <td>5.605273e+00</td>\n",
       "      <td>9.082315e-01</td>\n",
       "      <td>1.294372e-01</td>\n",
       "      <td>2.281464e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.001465</td>\n",
       "      <td>-4.301261e+01</td>\n",
       "      <td>-4.630812e+01</td>\n",
       "      <td>-4.291965e+01</td>\n",
       "      <td>-4.578092e+01</td>\n",
       "      <td>-4.450783e+01</td>\n",
       "      <td>-2.917431e+00</td>\n",
       "      <td>-3.078541e+00</td>\n",
       "      <td>-4.112710e-01</td>\n",
       "      <td>8.795677e+00</td>\n",
       "      <td>1.646070e+01</td>\n",
       "      <td>7.910000e+00</td>\n",
       "      <td>7.525035e+00</td>\n",
       "      <td>1.028792e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.001953</td>\n",
       "      <td>-6.406634e+01</td>\n",
       "      <td>-6.882520e+01</td>\n",
       "      <td>-6.631395e+01</td>\n",
       "      <td>-6.721487e+01</td>\n",
       "      <td>-6.648522e+01</td>\n",
       "      <td>-2.845243e+00</td>\n",
       "      <td>-1.015110e-01</td>\n",
       "      <td>4.815097e-01</td>\n",
       "      <td>1.029361e+01</td>\n",
       "      <td>2.246314e+01</td>\n",
       "      <td>1.194985e+01</td>\n",
       "      <td>1.084512e+01</td>\n",
       "      <td>1.351084e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       time          DLI5          DLI6          DLI7          DLI8  \\\n",
       "0  0.000000 -1.236668e-13 -1.389134e-13 -4.235165e-14 -1.694066e-13   \n",
       "1  0.000488 -2.430374e+01 -2.510319e+01 -2.527172e+01 -2.646931e+01   \n",
       "2  0.000977 -3.712937e+01 -3.905014e+01 -3.722450e+01 -4.003176e+01   \n",
       "3  0.001465 -4.301261e+01 -4.630812e+01 -4.291965e+01 -4.578092e+01   \n",
       "4  0.001953 -6.406634e+01 -6.882520e+01 -6.631395e+01 -6.721487e+01   \n",
       "\n",
       "           DLI9          DRI1          DRI2          DRI3          DRI4  \\\n",
       "0 -1.219727e-13  1.439956e-14  1.118083e-13  2.710505e-14  1.677125e-13   \n",
       "1 -2.484184e+01 -7.125326e+00 -7.385539e+00 -6.276301e+00 -2.353287e+00   \n",
       "2 -3.814335e+01 -6.721862e+00 -7.743634e+00 -4.963594e+00  2.268919e+00   \n",
       "3 -4.450783e+01 -2.917431e+00 -3.078541e+00 -4.112710e-01  8.795677e+00   \n",
       "4 -6.648522e+01 -2.845243e+00 -1.015110e-01  4.815097e-01  1.029361e+01   \n",
       "\n",
       "           DRI5          DRI6          DRI7          DRI8  \n",
       "0 -1.736418e-14 -4.235165e-15 -4.065758e-14 -4.065758e-14  \n",
       "1 -1.578070e+00 -2.794631e+00 -3.933598e+00 -2.874039e+00  \n",
       "2  5.605273e+00  9.082315e-01  1.294372e-01  2.281464e+00  \n",
       "3  1.646070e+01  7.910000e+00  7.525035e+00  1.028792e+01  \n",
       "4  2.246314e+01  1.194985e+01  1.084512e+01  1.351084e+01  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = raw.to_data_frame(picks = selected_picks)\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4d96c0fa-0b3e-4180-b9b5-e04e4dc5f29f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   -1.236668e-13\n",
       "1   -2.430374e+01\n",
       "2   -3.712937e+01\n",
       "3   -4.301261e+01\n",
       "4   -6.406634e+01\n",
       "Name: DLI5, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df = raw_df['DLI5']\n",
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "c184403d-ed03-4c48-a755-8ada3cfe063e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(61440, 14)\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167cee07-1792-4255-907e-5637d0bb6c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "z_score_baseline(processed_info_dir, ztmin, ztmax, sub_ids_lst[0], "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "898b9175-d17e-4a96-b293-35e961b1770f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de1b775-f29e-48ca-9644-9e17ba44df77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68263df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "        data_epo=combined_data_trials\n",
    "        print(data_epo.shape)\n",
    "        data_epo_zscore=np.copy(data_epo)\n",
    "        for i in range(data_epo.shape[0]): # for each epoch\n",
    "            for j in range(data_epo.shape[1]): # for each channel\n",
    "                # compute mean and std of baseline\n",
    "                base_mean = np.mean(data_epo[i,j,:len_baseline_samples])\n",
    "                base_std = np.std(data_epo[i,j,:len_baseline_samples])\n",
    "\n",
    "                # compute z-scored data from baseline stats\n",
    "                data_epo_zscore[i,j,:] = (data_epo[i,j,:]-base_mean)/base_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9295d303",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (1711115917.py, line 30)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[7], line 30\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(sub_fname)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "df_mean_all=[]\n",
    "df_peak_all=[]\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ LOAD PROCESSED INFO @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "    \n",
    "\n",
    "    # # Load in pain rating for each stimuli - pain ratings from Intracranial subjects are WIP\n",
    "    # pain_ratings = scio.loadmat(os.path.join(processed_info_dir,sub_num+'_pain_ratings.mat'))\n",
    "    # pain_ratings = pain_ratings['pain_ratings'].tolist()[0]\n",
    "    # print(f\"*pain_ratings.size = {len(pain_ratings)}*\\n\")\n",
    "\n",
    "    # # Load in drop log for bad trials\n",
    "    # drop_log = scio.loadmat(os.path.join(processed_info_dir,sub_num+'_drop_log.mat'))\n",
    "    # drop_log = drop_log['drop_log'] # leave as array\n",
    "    # print(f\"*drop_log.size = {drop_log.shape[0]}*\\n\")\n",
    "\n",
    "        display.clear_output(wait=True)\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ COMPUTE PSD FROM RAW FIF @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "\n",
    "    # TODO: implement following tutorial for psd from raw.fif\n",
    "    # https://mne.tools/0.21/auto_examples/time_frequency/plot_compute_raw_data_spectrum.html\n",
    "    # done\n",
    "\n",
    "    \n",
    "    # Extract PSD into each frequency band of interest\n",
    "    broad1 = epochs.compute_psd(fmin=1., fmax=55., n_jobs=-1) \n",
    "    broad2 = epochs.compute_psd(fmin=65., fmax=MUA_max., n_jobs=-1) # avoid the notch filter\n",
    "    delta = epochs.compute_psd(fmin=1., fmax=4., n_jobs=-1)\n",
    "    theta = epochs.compute_psd(fmin=4., fmax=8., n_jobs=-1)\n",
    "    alpha = epochs.compute_psd(fmin=8., fmax=13., n_jobs=-1)\n",
    "    beta = epochs.compute_psd(fmin=13., fmax=30., n_jobs=-1)\n",
    "    low_gamma = epochs.compute_psd(fmin=30., fmax=55., n_jobs=-1)\n",
    "    high_gamma = epochs.compute_psd(fmin=65., fmax=100., n_jobs=-1)\n",
    "    MUA = fmin = 100, fmax = MUA_max\n",
    "    \n",
    "    # Get data (shape = epochs x channels x powers in each band (1 hz width))\n",
    "    psds_broad1 = broad1.get_data()\n",
    "    psds_broad2 = broad2.get_data()\n",
    "    psds_delta = delta.get_data()\n",
    "    psds_theta = theta.get_data()\n",
    "    psds_alpha = alpha.get_data()\n",
    "    psds_beta = beta.get_data() \n",
    "    psds_low_gamma = low_gamma.get_data()\n",
    "    psds_high_gamma = high_gamma.get_data()\n",
    "\n",
    "    # Get average power across bands (shape = epochs x channels)\n",
    "    avg_psd_broad1 = np.mean(psds_broad1, axis=2)\n",
    "    avg_psd_broad2 = np.mean(psds_broad1, axis=2)\n",
    "    avg_psd_delta = np.mean(psds_delta, axis=2)\n",
    "    avg_psd_theta = np.mean(psds_theta, axis=2)\n",
    "    avg_psd_alpha = np.mean(psds_alpha, axis=2)\n",
    "    avg_psd_beta = np.mean(psds_beta, axis=2)\n",
    "    avg_psd_low_gamma = np.mean(psds_low_gamma, axis=2)\n",
    "    avg_psd_high_gamma = np.mean(psds_high_gamma, axis=2)\n",
    "\n",
    "    # Combine broad spectrum powers and take the trace\n",
    "    avg_psd_broad = np.mean( np.array([avg_psd_broad1, avg_psd_broad2 ]), axis=0 )\n",
    "    trace_avg_psd_broad = np.trace(avg_psd_broad)\n",
    "    print(\"Trace of the average broad spectrum power: \",trace_avg_psd_broad)\n",
    "\n",
    "    # Get band PSD's normalized by the trace of the broad spectrum and convert to dB\n",
    "    avg_psd_delta = 10 * np.log10(avg_psd_delta / trace_avg_psd_broad)\n",
    "    avg_psd_theta = 10 * np.log10(avg_psd_theta / trace_avg_psd_broad)\n",
    "    avg_psd_alpha = 10 * np.log10(avg_psd_alpha / trace_avg_psd_broad)\n",
    "    avg_psd_beta = 10 * np.log10(avg_psd_beta / trace_avg_psd_broad)\n",
    "    avg_psd_low_gamma = 10 * np.log10(avg_psd_low_gamma / trace_avg_psd_broad)\n",
    "    avg_psd_high_gamma = 10 * np.log10(avg_psd_high_gamma / trace_avg_psd_broad)\n",
    "\n",
    "    avg_psd_arr = np.array((avg_psd_delta, avg_psd_theta, \n",
    "                            avg_psd_alpha, avg_psd_beta,\n",
    "                            avg_psd_low_gamma, avg_psd_high_gamma))\n",
    "    \n",
    "    # # TODO: you may or may not need to use the following code block but im leaving it here just in case \n",
    "    # data_chs_array = np.zeros((len(roi_names),len(epo_times),Fs))\n",
    "    # data_trials_tmp=np.zeros((len(epo_times),Fs))\n",
    "    # for j,raw in enumerate(raw_objects_lst): # for each roi\n",
    "    #     extracted_data_tmp=raw.data\n",
    "    #     for i in range(len(epo_times)): # for each trial\n",
    "    #         #all vertices\n",
    "    #         all_vertices_tmp=extracted_data_tmp[:,int(epo_times[i]+tmin*Fs):int(epo_times[i]+tmax*Fs)]\n",
    "    #         #averaged vertices\n",
    "    #         vertices_averaged_tmp=np.mean(all_vertices_tmp,axis=0)\n",
    "    #         #store trials x time data in numpy array\n",
    "    #         data_trials_tmp[i,:] = vertices_averaged_tmp\n",
    "    #     data_chs_array[j,...] = data_trials_tmp\n",
    "    # # data in Array format \n",
    "    # data_chs_corrected_array=np.transpose(data_chs_array,(1,0,2))\n",
    "\n",
    "    # Z-score data by for loop\n",
    "    # TODO: z-score the data on a segment of the last-day data, and exclude that segment from the last-day data to be analyzed\n",
    "    data_epo=combined_data_trials\n",
    "    print(data_epo.shape)\n",
    "    data_epo_zscore=np.copy(data_epo)\n",
    "    for i in range(data_epo.shape[0]): # for each epoch\n",
    "        for j in range(data_epo.shape[1]): # for each channel\n",
    "            # compute mean and std of baseline\n",
    "            base_mean = np.mean(data_epo[i,j,:len_baseline_samples])\n",
    "            base_std = np.std(data_epo[i,j,:len_baseline_samples])\n",
    "        \n",
    "            # compute z-scored data from baseline stats\n",
    "            data_epo_zscore[i,j,:] = (data_epo[i,j,:]-base_mean)/base_std\n",
    "\n",
    "    # compare data before and after z-score\n",
    "    print(data_epo[0,0,len_baseline_samples:len_baseline_samples+10],'\\n')\n",
    "    print(data_epo_zscore[0,0,len_baseline_samples:len_baseline_samples+10])\n",
    "\n",
    "    # TODO: epochs object will just have z-scored day 1 data and last day data\n",
    "    # Create info object required for epochs object\n",
    "    info = mne.create_info(roi_names, sfreq=Fs,ch_types='eeg')\n",
    "    data_epo = data_chs_corrected_array\n",
    "    # Create events array for Epochs object\n",
    "    # TODO: 0*i means the start time from each piece of data will just be 0\n",
    "    # because we dont have any epochs in the data, we are treating the entire thing\n",
    "    # as one epoch.\n",
    "    # NOTE: the i in the last term of the list should just populate 0 and then 1,\n",
    "    # indicating day 1 as label of 0 and last day as label of 1.\n",
    "    \n",
    "    events=np.array([[0*i,0,i] for i in range(len(conditions))])\n",
    "\n",
    "    zepochs = mne.EpochsArray(data=data_epo_zscore,\n",
    "                              info=info,\n",
    "                              tmin=tmin,\n",
    "                              events=events,\n",
    "                              event_id=event_ids_dict,\n",
    "                              baseline=(None,bmax),\n",
    "                              )\n",
    "    print(zepochs)\n",
    "    epochs=zepochs \n",
    "    del zepochs\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "    \n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ CONSTRUCT TFR OBJECT FOR DF @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "    \n",
    "    # TODO: do not implement ERDS, leaving this code here to figure out how to use it to set up the dataframe\n",
    "    tfr = tfr_multitaper(\n",
    "        epochs,\n",
    "        freqs=freqs,\n",
    "        n_cycles=freqs,\n",
    "        use_fft=True,\n",
    "        return_itc=False,\n",
    "        average=False,\n",
    "        decim=2,\n",
    "    )\n",
    "    tfr.crop(tmin, tmax).apply_baseline(baseline, mode=\"percent\")\n",
    "\n",
    "    for event in event_ids:\n",
    "        # select desired epochs for visualization\n",
    "        tfr_ev = tfr[event]\n",
    "        fig, axes = plt.subplots(\n",
    "            1, 4, figsize=(12, 4), gridspec_kw={\"width_ratios\": [10, 10, 10, 1]}\n",
    "        )\n",
    "        for ch, ax in enumerate(axes[:-1]):  # for each channel\n",
    "            # positive clusters\n",
    "            _, c1, p1, _ = pcluster_test(tfr_ev.data[:, ch], tail=1, **kwargs)\n",
    "            # negative clusters\n",
    "            _, c2, p2, _ = pcluster_test(tfr_ev.data[:, ch], tail=-1, **kwargs)\n",
    "\n",
    "            # note that we keep clusters with p <= 0.05 from the combined clusters\n",
    "            # of two independent tests; in this example, we do not correct for\n",
    "            # these two comparisons\n",
    "            c = np.stack(c1 + c2, axis=2)  # combined clusters\n",
    "            p = np.concatenate((p1, p2))  # combined p-values\n",
    "            mask = c[..., p <= 0.05].any(axis=-1)\n",
    "\n",
    "            # plot TFR (ERDS map with masking)\n",
    "            tfr_ev.average().plot(\n",
    "                [ch],\n",
    "                cmap=\"RdBu\",\n",
    "                cnorm=cnorm,\n",
    "                axes=ax,\n",
    "                colorbar=False,\n",
    "                show=False,\n",
    "                mask=mask,\n",
    "                mask_style=\"mask\",\n",
    "            )\n",
    "\n",
    "            ax.set_title(epochs.ch_names[ch], fontsize=10)\n",
    "            ax.axvline(0, linewidth=1, color=\"black\", linestyle=\":\")  # event\n",
    "            if ch != 0:\n",
    "                ax.set_ylabel(\"\")\n",
    "                ax.set_yticklabels(\"\")\n",
    "        fig.colorbar(axes[0].images[-1], cax=axes[-1]).ax.set_yscale(\"linear\")\n",
    "        fig.suptitle(f\"ERDS ({event})\")\n",
    "        plt.show()\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ SET UP PANDAS DATAFRAMES @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "\n",
    "    #Export tfr to pandas df in seconds\n",
    "    df = tfr.to_data_frame(time_format=None, long_format=False)\n",
    "    df.head()\n",
    "\n",
    "    #Plot with confidence bands \n",
    "    df = tfr.to_data_frame(time_format=None, long_format=True)\n",
    "\n",
    "    # Map to frequency bands:\n",
    "    freq_bounds = {'_': 0,\n",
    "                   'delta': 4,\n",
    "                   'theta': 7,\n",
    "                   'alpha': 13,\n",
    "                   'beta': 35,\n",
    "                   'low-gamma': 55,\n",
    "                   'notch': 65,\n",
    "                   'high-gamma': 100,\n",
    "                   'MUA': 500\n",
    "                  }\n",
    "\n",
    "    df['band'] = pd.cut(df['freq'], list(freq_bounds.values()),\n",
    "                        labels=list(freq_bounds)[1:])\n",
    "\n",
    "    # Filter to retain only relevant frequency bands:\n",
    "    freq_bands_of_interest = [\n",
    "                              # 'delta', \n",
    "                              'theta', \n",
    "                              'alpha', \n",
    "                              'beta', \n",
    "                              'low-gamma', \n",
    "                              'high-gamma',\n",
    "                              'MUA',\n",
    "        ]\n",
    "    df = df[df.band.isin(freq_bands_of_interest)]\n",
    "    df['band'] = df['band'].cat.remove_unused_categories()\n",
    "\n",
    "    # Order channels for plotting:\n",
    "    df['channel'] = df['channel'].cat.reorder_categories(tuple(roi_names),\n",
    "                                                         ordered=True)\n",
    "\n",
    "    g = sns.FacetGrid(df, row='band', col='channel', margin_titles=True)\n",
    "    g.map(sns.lineplot, 'time', 'value', n_boot=10)\n",
    "    axline_kw = dict(color='black', linestyle='dashed', linewidth=0.5, alpha=0.5)\n",
    "    g.map(plt.axhline, y=0, **axline_kw)\n",
    "    g.map(plt.axvline, x=0, **axline_kw)\n",
    "    g.set(xlim=(tmin+0.5,tmax-0.5))\n",
    "    # g.set(ylim=(None, 3))\n",
    "    g.set_axis_labels(\"Time (s)\", \"Z-scored Band Power (uV^2)\")\n",
    "    g.set_titles(col_template=\"{col_name}\", row_template=\"{row_name}\")\n",
    "    g.add_legend(ncol=2, loc='lower center')\n",
    "    g.fig.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.08)\n",
    "    plt.show();\n",
    "\n",
    "    display.clear_output(wait=True)\n",
    "\n",
    "    # Plot the ERDS based on average, peak amplitude, and area-under-the-curve (AUC), one by one.\n",
    "    df_mean = (df.query(f\"time > {tmin}\")\n",
    "                 .groupby(['epoch','band','channel'])[['value']]\n",
    "                 .mean()\n",
    "                 .dropna(subset=['value']) # needed to remove taking average over non-numeric values\n",
    "                 .reset_index())\n",
    "\n",
    "    df_mean_all.append(df_mean) #when it loops through each subject, will save the mean in the array\n",
    "\n",
    "    df_peak = (df.query(f\"time > {tmin} & time < {tmax}\")\n",
    "                 .groupby(['epoch','band','channel'])[['value']]\n",
    "                 .max() # peak amplitude\n",
    "                 .dropna(subset=['value']) # needed to remove taking average over non-numeric values\n",
    "                 .reset_index())\n",
    "    df_peak_all.append(df_peak) #when it loops through each subject, will save the peak\n",
    "\n",
    "    # @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ EVALUATE AND SAVE STATS OUTPUTS @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ #\n",
    "    stats_plot(df_mean, save_dir, 'mean')\n",
    "    stats_plot(df_peak, save_dir, 'peak')\n",
    "    \n",
    "    display.clear_output(wait=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1794755a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae315a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42048b49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eegenv",
   "language": "python",
   "name": "eegenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
